{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 8, Loss: 0.2534204125404358\n",
      "Epoch 1, Step 16, Loss: 0.2427542358636856\n",
      "Epoch 1, Step 24, Loss: 0.19090424478054047\n",
      "Epoch 1, Step 32, Loss: 0.15652111172676086\n",
      "Epoch 1, Step 40, Loss: 0.1298975646495819\n",
      "Epoch 1, Step 48, Loss: 0.1380399465560913\n",
      "Epoch 1, Step 56, Loss: 0.0988500788807869\n",
      "Epoch 1, Step 64, Loss: 0.10447648912668228\n",
      "Epoch 1, Step 72, Loss: 0.10988999158143997\n",
      "Epoch 1, Step 80, Loss: 0.0863654688000679\n",
      "Epoch 1, Step 88, Loss: 0.07435011863708496\n",
      "Epoch 1, Step 96, Loss: 0.08043905347585678\n",
      "Epoch 1, Step 104, Loss: 0.0729428231716156\n",
      "Epoch 1, Step 112, Loss: 0.06611660122871399\n",
      "Epoch 1, Step 120, Loss: 0.06419971585273743\n",
      "Epoch 1, Step 129, Loss: 0.041641946882009506\n",
      "Epoch 1, Step 138, Loss: 0.04795615002512932\n",
      "Epoch 1, Step 146, Loss: 0.04832075536251068\n",
      "Epoch 1, Step 154, Loss: 0.04018230736255646\n",
      "Epoch 1, Step 162, Loss: 0.038446441292762756\n",
      "Epoch 1, Step 171, Loss: 0.04135090112686157\n",
      "Epoch 1, Step 179, Loss: 0.0362999327480793\n",
      "Epoch 1, Step 188, Loss: 0.04362837225198746\n",
      "Epoch 1, Step 196, Loss: 0.043539948761463165\n",
      "Epoch 1, Step 204, Loss: 0.02901392988860607\n",
      "Epoch 1, Step 212, Loss: 0.03040042333304882\n",
      "Epoch 1, Step 220, Loss: 0.03196967393159866\n",
      "Epoch 1, Step 228, Loss: 0.025968942791223526\n",
      "Epoch 1, Step 236, Loss: 0.026074199005961418\n",
      "Epoch 1, Step 244, Loss: 0.029097024351358414\n",
      "Epoch 1, Step 252, Loss: 0.0232987143099308\n",
      "Epoch 1, Step 260, Loss: 0.02217733860015869\n",
      "Epoch 1, Step 268, Loss: 0.025064382702112198\n",
      "Epoch 1, Step 276, Loss: 0.02266286127269268\n",
      "Epoch 1, Step 284, Loss: 0.020653456449508667\n",
      "Epoch 1, Step 292, Loss: 0.021698608994483948\n",
      "Epoch 1, Step 300, Loss: 0.01614527776837349\n",
      "Epoch 1, Step 309, Loss: 0.017530230805277824\n",
      "Epoch 1, Step 317, Loss: 0.01712515950202942\n",
      "Epoch 1, Step 325, Loss: 0.015006808564066887\n",
      "Epoch 1, Step 334, Loss: 0.010705400258302689\n",
      "Epoch 1, Step 342, Loss: 0.01546785980463028\n",
      "Epoch 1, Step 350, Loss: 0.016666170209646225\n",
      "Epoch 1, Step 358, Loss: 0.01388818584382534\n",
      "Epoch 1, Step 367, Loss: 0.018270181491971016\n",
      "Epoch 1, Step 376, Loss: 0.011564438231289387\n",
      "Epoch 1, Step 384, Loss: 0.010992908850312233\n",
      "Epoch 1, Step 392, Loss: 0.012035555206239223\n",
      "Epoch 1, Step 401, Loss: 0.012392832897603512\n",
      "Epoch 1, Step 409, Loss: 0.010758270509541035\n",
      "Epoch 1, Step 417, Loss: 0.009673514403402805\n",
      "Early stopping at Epoch 1, Step 417, Loss: 0.009673514403402805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('tokenizer_scidocs\\\\tokenizer_config.json',\n",
       " 'tokenizer_scidocs\\\\special_tokens_map.json',\n",
       " 'tokenizer_scidocs\\\\vocab.txt',\n",
       " 'tokenizer_scidocs\\\\added_tokens.json',\n",
       " 'tokenizer_scidocs\\\\tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import torch\n",
    "from torch.nn import CosineSimilarity\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load SCIDOCS corpus\n",
    "corpus = load_dataset(\"BeIR/scidocs\", \"corpus\")\n",
    "\n",
    "# Initialize tokenizer and models, move models to device\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "query_model = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "document_model = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\n",
    "# Cosine similarity function\n",
    "cosine_sim = CosineSimilarity(dim=1)\n",
    "\n",
    "# Function to encode text in batches\n",
    "def encode_text_batch(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=64).to(device)\n",
    "    embeddings = model(**inputs).last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "    return embeddings\n",
    "\n",
    "# Function to create ICT and BFS pairs\n",
    "def create_ict_pair(text):\n",
    "    sentences = text.split(\". \")\n",
    "    if len(sentences) <= 1:\n",
    "        return None, None\n",
    "    query = sentences.pop(random.randint(0, len(sentences) - 1))\n",
    "    document = \". \".join(sentences)\n",
    "    return query, document\n",
    "\n",
    "def create_bfs_pair(text):\n",
    "    sections = text.split(\". \")\n",
    "    if len(sections) <= 1:\n",
    "        return None, None\n",
    "    query = sections[0]\n",
    "    document = \". \".join(sections[1:])\n",
    "    return query, document\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 5e-6  # Lower learning rate\n",
    "batch_size = 8\n",
    "epochs = 2\n",
    "loss_threshold = 0.01  # Higher threshold for early stopping\n",
    "\n",
    "# Initialize optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.Adam(list(query_model.parameters()) + list(document_model.parameters()), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-7, verbose=True)\n",
    "\n",
    "# Training loop with early stopping\n",
    "for epoch in range(epochs):\n",
    "    queries = []\n",
    "    documents = []\n",
    "    early_stop = False  # Flag for early stopping\n",
    "    \n",
    "    for idx, item in enumerate(corpus['corpus']):\n",
    "        document_text = item['text']\n",
    "        \n",
    "        # Alternate between ICT and BFS\n",
    "        if random.random() > 0.5:\n",
    "            query, document = create_ict_pair(document_text)\n",
    "        else:\n",
    "            query, document = create_bfs_pair(document_text)\n",
    "\n",
    "        # Append to batch if pair is valid\n",
    "        if query and document:\n",
    "            queries.append(query)\n",
    "            documents.append(document)\n",
    "        \n",
    "        # Process in batches\n",
    "        if len(queries) == batch_size:\n",
    "            # Encode queries and documents\n",
    "            query_embeddings = encode_text_batch(queries, tokenizer, query_model)\n",
    "            document_embeddings = encode_text_batch(documents, tokenizer, document_model)\n",
    "            \n",
    "            # Calculate similarity and loss (margin-based contrastive loss)\n",
    "            positive_similarity = cosine_sim(query_embeddings, document_embeddings)\n",
    "            loss = -torch.log(positive_similarity + 1e-8).mean()\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}, Step {idx + 1}, Loss: {loss.item()}\")\n",
    "            \n",
    "            # Check for early stopping\n",
    "            if loss.item() < loss_threshold:\n",
    "                print(f\"Early stopping at Epoch {epoch + 1}, Step {idx + 1}, Loss: {loss.item()}\")\n",
    "                early_stop = True\n",
    "                break\n",
    "            \n",
    "            # Update learning rate based on loss\n",
    "            scheduler.step(loss)\n",
    "            \n",
    "            # Clear batch\n",
    "            queries = []\n",
    "            documents = []\n",
    "\n",
    "    if early_stop:\n",
    "        break  # Stop training if early stop condition is met\n",
    "\n",
    "# Save models\n",
    "query_model.save_pretrained(\"query_model_scidocs\")\n",
    "document_model.save_pretrained(\"document_model_scidocs\")\n",
    "tokenizer.save_pretrained(\"tokenizer_scidocs\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
