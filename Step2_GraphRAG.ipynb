{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3526: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# LangChain's core runnables for orchestrating tasks in workflows\n",
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "# LangChain's core components for building custom prompts, handling messages, and parsing outputs\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Typing Imports\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Integrating LangChain with Neo4j, which can be useful for tasks like combining graph databases and vector stores for advanced AI workflows.\n",
    "# For example:\n",
    "# We can use Neo4jGraph to retrieve structured graph data from Neo4j\n",
    "# We can store and query document embeddings using Neo4jVector\n",
    "# We can leverage LLMGraphTransformer to help the LLM reason about relationships within the graph\n",
    "# We can use remove_lucene_chars to ensure that queries passed into Neo4j are well-formatted and don’t cause issues with search.\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "# Document Loaders and Text Splitters\n",
    "# from langchain.document_loaders import WikipediaLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# LangChain components that interface with OpenAI models\n",
    "# ChatOpenAI handles interactive conversations with a language model\n",
    "# OpenAIEmbeddings transform text into vectors, stores and compares the semantic meaning of user inputs or documents in a vector store like Neo4jVector.\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Neo4j & Graph Visualization\n",
    "# To establish a connection with a Neo4j database and handling the graph database by running Cypher queries, interacting with nodes and relationships\n",
    "from neo4j import GraphDatabase\n",
    "# To visually represent the graph data retrieved from Neo4j\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "\n",
    "# FAISS (Facebook AI Similarity Search) stores text embeddings and then retrieves similar documents based on a query\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Chains for QA by combining a retrieval mechanism (like FAISS) with a language model\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "import warnings\n",
    "import textwrap\n",
    "\n",
    "#colab imports if running in Google colab\n",
    "try:\n",
    "  import google.colab\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "except:\n",
    "  pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The purpose of this notebook is to get an result for the specific course Data-Driven Marketing. It is for us to understand the model behaviour and adjust it accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\" # Apply your own key\n",
    "os.environ[\"NEO4J_URI\"] = '' # Apply your own URI\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\" # by default or use your own\n",
    "os.environ[\"NEO4J_PASSWORD\"] = '' # Apply your own password\n",
    "\n",
    "# Create a connection to the Neo4j database\n",
    "# graph = Neo4jGraph()\n",
    "graph = Neo4jGraph(url=os.environ[\"NEO4J_URI\"], username=os.environ[\"NEO4J_USERNAME\"], password=os.environ[\"NEO4J_PASSWORD\"]) # Explicitly pass the connection details to Neo4jGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatOpenAI(temperature=0, model_name=\"gpt-4-0125-preview\") # gpt-4-0125-preview occasionally has issues but in theory you would want to use the most capable model to construct the graph\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example on how the LLM will respond to a prompt which it has no knowledge on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we reuse the query from User_query_based_filter.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of LLM without RAG process: \n",
      "\n",
      "Creating a study plan for a course like Data-driven Marketing is a great way to\n",
      "ensure you cover all necessary material, understand the concepts thoroughly, and\n",
      "apply them effectively. Here's a step-by-step guide to help you create a\n",
      "comprehensive study plan:  ### 1. Understand the Course Structure - **Review the\n",
      "Syllabus:** Start by thoroughly reviewing the course syllabus. Note down the key\n",
      "topics, assignments, project work, and exams. - **Identify Key Modules:** Break\n",
      "down the syllabus into main modules or sections. This could be based on weeks,\n",
      "topics, or types of marketing strategies discussed.  ### 2. Set Clear Objectives\n",
      "- **Learning Goals:** Define what you want to achieve by the end of the course.\n",
      "This could range from understanding specific marketing models to being able to\n",
      "analyze and interpret data effectively. - **Skill Development:** Identify the\n",
      "skills you aim to develop, such as analytical thinking, data visualization, or\n",
      "strategic planning.  ### 3. Allocate Time Wisely - **Study Schedule:** Create a\n",
      "weekly study schedule. Allocate time based on your current commitments and the\n",
      "difficulty of topics. Remember to include time for revision and practice. -\n",
      "**Flexibility:** Ensure your plan is flexible to accommodate unexpected changes\n",
      "or additional time for challenging topics.  ### 4. Diversify Learning Resources\n",
      "- **Course Material:** Start with the provided course material, such as\n",
      "textbooks, lecture notes, and online resources. - **Supplementary Resources:**\n",
      "Look for additional resources like online tutorials, industry reports, case\n",
      "studies, and academic papers to gain a broader understanding.  ### 5. Engage\n",
      "with Practical Elements - **Hands-On Practice:** Data-driven marketing requires\n",
      "practical application. Engage with any datasets provided, use analytical tools,\n",
      "and try to work on real-life case studies. - **Projects and Assignments:**\n",
      "Allocate sufficient time for projects and assignments. These are crucial for\n",
      "applying theoretical knowledge.  ### 6. Collaborate and Communicate - **Study\n",
      "Groups:** Join or form study groups. Discussing topics with peers can enhance\n",
      "understanding and expose you to different perspectives. - **Networking:** Engage\n",
      "with professionals in the field through social media, forums, or events. This\n",
      "can provide insights into industry practices and trends.  ### 7. Review and\n",
      "Revise - **Regular Reviews:** Schedule weekly reviews to assess your\n",
      "understanding of the topics covered. Adjust your study plan based on your\n",
      "progress. - **Practice Tests:** If available, take practice tests to evaluate\n",
      "your readiness for exams. This can also help in identifying areas that need more\n",
      "focus.  ### 8. Take Care of Yourself - **Breaks:** Include short breaks in your\n",
      "study sessions to avoid burnout. - **Well-being:** Maintain a healthy lifestyle,\n",
      "with adequate sleep, nutrition, and physical activity, to keep your mind sharp.\n",
      "### 9. Reflect and Adjust - **Reflect on Learning:** Regularly reflect on what\n",
      "you’ve learned and how you can apply it. - **Adjust Your Plan:** Be prepared to\n",
      "adjust your study plan based on your progress and feedback from assignments or\n",
      "exams.  ### Tools and Resources - **Digital Tools:** Utilize digital tools like\n",
      "Google Calendar for scheduling, Trello or Notion for task management, and\n",
      "educational platforms like Coursera or edX for additional courses. - **Analytics\n",
      "Tools:** Familiarize yourself with analytics tools relevant to your course, such\n",
      "as Google Analytics, Tableau, or Python for data analysis, as these skills are\n",
      "often crucial in data-driven marketing.  By following these steps and staying\n",
      "committed to your study plan, you'll be well on your way to mastering Data-\n",
      "driven Marketing. Remember, the key to success is consistency, engagement, and\n",
      "the willingness to explore and apply new concepts.\n"
     ]
    }
   ],
   "source": [
    "print('Example of LLM without RAG process: \\n')\n",
    "response = llm(\"Can you help me make a study plan for the course Data-driven marketing?\").content\n",
    "wrapped_response = textwrap.fill(response, width=80)\n",
    "print(wrapped_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_file(filename):\n",
    "    # Load text data from a .txt file\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Chunk 1:\n",
      "Course Overview:\n",
      "Title: Data-Driven Marketing | Description: In today’s environment, marketing or business analysts require tools and techniques to both quantify the strategic value of marketing initiatives, and to maximize marketing campaign performance. This course aims to teach students concepts, methods and tools to demonstrate the return on investment (ROI) of marketing activities and to leverage on data and marketing analytics to make better and more informed marketing decisions. Course topics covered include marketing performance management, marketing metrics, data management, market response and diffusion models, market and customer segmentation models, analytic marketing and value driven segmentation, digital media marketing analytics, etc. Students will have access to | Subject: Computer Science\n",
      "\n",
      "Related Article 1:\n",
      "Title: Article 1\n",
      "Content: text: User behavior analytics (UBA) is a cybersecurity process about detection of insider threats, targeted attacks, and financial fraud that tracks a system's users. UBA looks at patterns of human behavior, and then analyzes them to detect anomalies that indicate potential threats. Big data platforms like Apache Hadoop are increasing UBA functionality by allowing them to analyze petabytes worth of data to detect insider threats and advanced persistent threats.\n",
      "\n",
      "Purpose \n",
      "UBA's purpose, according to Johna Till Johnson of Nemertes Research, is that \"Security systems provide so much information that it's tough to uncover information that truly indicates a potential for real attack. Analytics tools help make sense of the vast amount of data that SIEM, IDS/IPS, system logs, and other tools gather. UBA tools use a specialized type of security analytics that focuses on the behavior of systems and the people using them. UBA technology first evolved in the field of marketing, to help companies understand and predict consumer-buying patterns. But as it turns out, UBA can be extraordinarily useful in the security context too.\"\n",
      "\n",
      "See also\n",
      " Behavioral analytics\n",
      " Network behavior anomaly detection\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      "\n",
      "ABC's Of UBA\n",
      "\n",
      "Software | url: https://en.wikipedia.org/wiki/User%20behavior%20analytics | title: User behavior analytics\n",
      "\n",
      "\n",
      "In addition, the following article provides insights:\n",
      "\n",
      "Related Article 2:\n",
      "Title: Article 2\n",
      "Content: text: Software installed in medical devices is assessed for health and safety issues according to international standards.\n",
      "\n",
      "Safety classes \n",
      "Software classification is based on the potential for hazard(s) that could cause injury to the user or patient.\n",
      "\n",
      "Per [[I\n",
      "\n",
      "Document Chunk 2:\n",
      " for hazard(s) that could cause injury to the user or patient.\n",
      "\n",
      "Per [[IEC 62304|IEC 62304:2006] + A1:2015], the software can be divided into three separate classes:\n",
      " The SOFTWARE SYSTEM is software safety class A if:\n",
      "the SOFTWARE SYSTEM cannot contribute to a HAZARDOUS SITUATION; or\n",
      "the SOFTWARE SYSTEM can contribute to a HAZARDOUS SITUATION which does not result in unacceptable RISK after consideration of RISK CONTROL measures external to the SOFTWARE SYSTEM.\n",
      "The SOFTWARE SYSTEM is software safety class B if:\n",
      "the SOFTWARE SYSTEM can contribute to a HAZARDOUS SITUATION which results in unacceptable RISK after consideration of RISK CONTROL measures external to the SOFTWARE SYSTEM and the resulting possible HARM is non-SERIOUS INJURY.\n",
      "The SOFTWARE SYSTEM is software safety class C if:\n",
      "the SOFTWARE SYSTEM can contribute to a HAZARDOUS SITUATION which results in unacceptable RISK after consideration of RISK CONTROL measures external to the SOFTWARE SYSTEM and the resulting possible HARM is death or SERIOUS INJURY“\n",
      "\n",
      "Serious injury \n",
      "For the purpose of this classification, serious injury is defined as injury or illness that directly or indirectly is life threatening; results in permanent impairment of a body function or permanent damage to a body structure; or necessitates medical or surgical intervention to prevent permanent impairment of a body function or permanent damage to a body structure.\n",
      "\n",
      "References\n",
      "\n",
      "Software\n",
      "Occupational safety and health | url: https://en.wikipedia.org/wiki/Software%20safety%20classification | title: Software safety classification\n",
      "\n",
      "\n",
      "In addition, the following article provides insights:\n",
      "\n",
      "Related Article 3:\n",
      "Title: Article 3\n",
      "Content: text: To classify postoperative outcomes for epilepsy surgery, Jerome Engel proposed the following scheme, the Engel Epilepsy Surgery Outcome Scale, which has become the de facto standard when reporting results in the medical literature:\n",
      " Class I: Free of disabling seizures\n",
      " Class II: Rare disabling seizures (\"almost seizure-free\")\n",
      " Class III: Worthwhile improvement\n",
      " Class IV: No worthwhile improvement\n",
      "\n",
      "History\n",
      "\n",
      "Surgery for epilepsy patients has been used for over a century, but due to technological restrictions and insufficient knowledge of brain surgery, this treatment approach was relatively rare until the 1980s and 90s.  Prior to the 1980s, no classification system existed due to the lack of operations performed up\n",
      "\n",
      "Document Chunk 3:\n",
      ".  Prior to the 1980s, no classification system existed due to the lack of operations performed up until the time. As surgery as a treatment grew more prevalent, a classification system became a necessity. The appropriate evaluation of patients following epilepsy surgery is extremely important, as medical professionals must know the appropriate course of action to follow in order to achieve seizure freedom for patients.  Accordingly, the Engel classification guidelines were devised by UCLA neurologist Jerome Engel Jr. in 1987 and made public at the 1992 Palm Desert Conference on Epilepsy Surgery. The Engel classification system has since become the standard in reporting postoperative outcomes of epilepsy surgery.\n",
      "\n",
      "Overview\n",
      "\n",
      "In Engel's 1993 summary of the 1992 Palm Desert Conference on Epilepsy Surgery, he annotated his classification system with more detail.  The annotation was as follows:\n",
      " Class I: Seizure free or no more than a few early, nondisabling seizures; or seizures upon drug withdrawal only\n",
      " Class II: Disabling seizures occur rarely during a period of at least 2 years; disabling seizures may have been more frequent soon after surgery; nocturnal seizures\n",
      " Class III: Worthwhile improvement; seizure reduction for prolonged periods but less than 2 years\n",
      " Class IV: No worthwhile improvement; some reduction, no reduction, or worsening are possible\n",
      "\n",
      "Advantages\n",
      "\n",
      "The subjectivity of the Engel system leaves much of the postoperative class assignment process to the patients. While many have noted the disadvantages of a classification system where the patients are involved in determining the evaluation, others have praised it.  Proponents of the Engel classification guidelines argue that the patients are best able to perceive the worth of the operation because they are the ones experiencing the seizures before and after the treatment.\n",
      "\n",
      "Disadvantages\n",
      "\n",
      "As is the case for all current methods of reviewing epilepsy surgery outcomes, the Engel classification system has subjective components. A \"disabling seizure\" is subjective and can vary in definition from person to person. While one epileptic experiencing a seizure when driving a car may find the seizure \"disabling,\" the same magnitude of seizure may be interpreted as mild, and thus \"nondisabling,\" by an epileptic resting in bed. Every class other than class I is also subjective because there is no quantitative definition of what determines a rare occurrence or method to measure worthwhileness. One doctor and patient may consider 2 seizures in a year as a rare occurrence while another doctor may consider 10 in a year as rarely occurring. The worthwhileness of the operation is ambiguous because worth can\n",
      "\n",
      "Document Chunk 4:\n",
      " in a year as rarely occurring. The worthwhileness of the operation is ambiguous because worth can be interpreted differently by various patients and healthcare professionals. Keeping those caveats in mind, most neurologists and neurosurgeons who specialize in epilepsy would most likely agree, as would many epileptics and even laypeople, that any seizure that leads to a period of status epilepticus (seizure activity, especially of the tonic-clonic, or grand mal, type, for longer than about five to ten minutes, or more- some now say it should be as little as two- without an intervening return to normal, or any repeat seizures without a return to consciousness) is a medical emergency, objectively a major problem, and cannot be considered a satisfactory outcome (unless perhaps if the person had a fatal or very severe form of a neurodegenerative syndrome or other disease where such severe repeat seizures are not unusual, and there are a number of these diseases; even then, such an outcome is usually still not a cure, just an amelioration of a fatal condition or a very disabling condition). Continuing to have to endure a large number of tonic-clonic seizures (grand mal seizures) over a period of days, months, or even over the course of a year or two, would make it impossible to drive and very hard to hold a job away from home entailing much stress, and would pose limits on one's abilities to safely carry out the activities of daily living without at least some monitoring or assistance.\n",
      "\n",
      "The Engel classification system has been thought of as a cross-sectional grading system by medical professionals because it does not account for long term changes in patients.  It has been proposed that it would be more beneficial to reevaluate patients on an annual basis, and the International League Against Epilepsy (ILAE) devised a separate rating scale in 2001 that reevaluates patients on every annual anniversary of their surgery. The ILAE also developed their system in hopes of avoiding many of the subjective components found in the Engel system.\n",
      "\n",
      "References\n",
      "\n",
      "Medical terminology\n",
      "Neurological disorders\n",
      "Neurology\n",
      "Epilepsy | url: https://en.wikipedia.org/wiki/Engel%20classification | title: Engel classification\n",
      "\n",
      "\n",
      "In addition, the following article provides insights:\n",
      "\n",
      "Related Article 4:\n",
      "Title: Article 4\n",
      "Content: text: Unsupervised learning  is a type of algorithm that learns patterns from untagged data. The hope is that through\n",
      "\n",
      "Document Chunk 5:\n",
      " learning  is a type of algorithm that learns patterns from untagged data. The hope is that through mimicry, which is an important mode of learning in people, the machine is forced to build a compact internal representation of its world and then generate imaginative content from it. In contrast to supervised learning where data is tagged by an expert, e.g. as a \"ball\" or \"fish\", unsupervised methods exhibit self-organization that captures patterns as probability densities  or a combination of neural feature preferences. The other levels in the supervision spectrum are reinforcement learning where the machine is given only a numerical performance score as guidance, and semi-supervised learning where a smaller portion of the data is tagged. Two broad methods in Unsupervised Learning are Neural Networks and Probabilistic Methods.\n",
      "\n",
      "Neural networks\n",
      "\n",
      "Tasks vs. Methods \n",
      "\n",
      "Neural network tasks are often categorized as discriminative (recognition) or generative (imagination).  Often but not always, discriminative tasks use supervised methods and generative tasks use unsupervised (see Venn diagram); however, the separation is very hazy.  For example, object recognition favors supervised learning but unsupervised learning can also cluster objects into groups.  Furthermore, as progress marches onward some tasks employ both methods, and some tasks swing from one to another.  For example, image recognition started off as heavily supervised, but became hybrid by employing unsupervised pre-training, and then moved towards supervision again with the advent of dropout, relu, and adaptive learning rates.\n",
      "\n",
      "Training \n",
      "During the learning phase, an unsupervised network tries to mimic the data it's given and uses the error in its mimicked output to correct itself (ie. correct its weights & biases). This resembles the mimicry behavior of children as they learn a language.  Sometimes the error is expressed as a low probability that the erroneous output occurs, or it might be express as an unstable high energy state in the network.\n",
      "\n",
      "In contrast to Supervised method's dominant use of Backpropagation, Unsupervised Learning also employ other methods  including:  Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations.  See the table below for more details.\n",
      "\n",
      "Energy \n",
      "An energy function is a\n",
      "\n",
      "Document Chunk 6:\n",
      ".  See the table below for more details.\n",
      "\n",
      "Energy \n",
      "An energy function is a macroscopic measure of a network's activation state.  In Boltzmann machines, it plays the role of the Cost function.  This analogy with physics is inspired by Ludwig Boltzmann's analysis of a gas' macroscopic energy from the microscopic probabilities of particle motion p  eE/kT, where k is the Boltzmann constant and T is temperature. In the RBM network the relation is p = e−E / Z, where p & E vary over every possible activation pattern and Z =  e -E(pattern). To be more precise, p(a) = e-E(a) / Z, where a is an activation pattern of all neurons (visible and hidden). Hence, early neural networks bear the name Boltzmann Machine.  Paul Smolensky calls -E the Harmony. A network seeks low energy which is high Harmony.\n",
      "\n",
      "Networks \n",
      "This table shows connection diagrams of various unsupervised networks, the details of which will be given in the section Comparison of Network.  Of the networks bearing people's names, only Hopfield worked directly with neural networks.  Boltzmann and Helmholtz lived before the invention of artificial neural networks, but they did inspire the analytical methods that were used.\n",
      "\n",
      "History\n",
      "\n",
      "Specific Networks \n",
      "\n",
      "Here, we highlight some characteristics of each networks. Ferromagnetism inspired Hopfield networks, Boltzmann machines, and RBMs. A neuron correspond to an iron domain with binary magnetic moments Up and Down, and neural connections correspond to the domain's influence on each other. Symmetric connections enables a global energy formulation. During inference the network updates each state using the standard activation step function. Symmetric weights guarantees convergence to a stable activation pattern.\n",
      "\n",
      "Comparison of Networks \n",
      "\n",
      "Hebbian Learning, ART, SOM\n",
      "The classical example of unsupervised learning in the study of neural networks is Donald Hebb's principle, that is, neurons that fire together wire together. In Hebbian learning, the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons. A similar version that modifies synaptic weights takes into account the time between the action potentials (spike-timing-dependent plasticity or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such\n",
      "\n",
      "Document Chunk 7:\n",
      " STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.\n",
      "\n",
      "Among neural network models, the self-organizing map (SOM) and adaptive resonance theory (ART) are commonly used in unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing.\n",
      "\n",
      "Probabilistic methods \n",
      "Two of the main methods used in unsupervised learning are principal component and cluster analysis. Cluster analysis is used in unsupervised learning to group, or segment, datasets with shared attributes in order to extrapolate algorithmic relationships. Cluster analysis is a branch of machine learning that groups the data that has not been labelled, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. This approach helps detect anomalous data points that do not fit into either group.\n",
      "\n",
      "A central application of unsupervised learning is in the field of density estimation in statistics, though unsupervised learning encompasses many other domains involving summarizing and explaining data features. It can be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution  conditioned on the label  of input data; unsupervised learning intends to infer an a priori probability distribution .\n",
      "\n",
      "Approaches \n",
      "Some of the most common algorithms used in unsupervised learning include: (1) Clustering, (2) Anomaly detection, (3) Approaches for learning latent variable models. Each approach uses several methods as follows:\n",
      "\n",
      " Clustering methods include: hierarchical clustering, k-means, mixture models, DBSCAN, and OPTICS algorithm\n",
      " Anomaly detection methods include: Local Outlier Factor, and Isolation Forest\n",
      " Approaches for learning latent variable models such as Expectation–maximization algorithm (EM), Method of moments, and Blind signal separation techniques (Principal component analysis, Independent component analysis, Non-negative matrix factorization, Singular value decomposition)\n",
      "\n",
      "Method of moments \n",
      "One of the statistical approaches\n",
      "\n",
      "Document Chunk 8:\n",
      "ization, Singular value decomposition)\n",
      "\n",
      "Method of moments \n",
      "One of the statistical approaches for unsupervised learning is the method of moments. In the method of moments, the unknown parameters (of interest) in the model are related to the moments of one or more random variables, and thus, these unknown parameters can be estimated given the moments. The moments are usually estimated from samples empirically. The basic moments are first and second order moments. For a random vector, the first order moment is the mean vector, and the second order moment is the covariance matrix (when the mean is zero). Higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multi-dimensional arrays.\n",
      "\n",
      "In particular, the method of moments is shown to be effective in learning the parameters of latent variable models. Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. A highly practical example of latent variable models in machine learning is the topic modeling which is a statistical model for generating the words (observed variables) in the document based on the topic (latent variable) of the document. In the topic modeling, the words in the document are generated according to different statistical parameters when the topic of the document is changed. It is shown that method of moments (tensor decomposition techniques) consistently recover the parameters of a large class of latent variable models under some assumptions.\n",
      "\n",
      "The Expectation–maximization algorithm (EM) is also one of the most practical methods for learning latent variable models. However, it can get stuck in local optima, and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model. In contrast, for the method of moments, the global convergence is guaranteed under some conditions.\n",
      "\n",
      "See also \n",
      " Automated machine learning\n",
      " Cluster analysis\n",
      " Anomaly detection\n",
      " Expectation–maximization algorithm\n",
      " Generative topographic map\n",
      " Meta-learning (computer science)\n",
      " Multivariate analysis\n",
      " Radial basis function network\n",
      "Weak supervision\n",
      "\n",
      "References\n",
      "\n",
      "Further reading \n",
      " \n",
      " \n",
      "\n",
      "  (This book focuses on unsupervised learning in neural networks)\n",
      "\n",
      " \n",
      "Machine learning | url: https://en.wikipedia.org/wiki/Unsupervised%20learning | title: Unsupervised learning\n",
      "\n",
      "\n",
      "In addition, the following article provides insights:\n",
      "\n",
      "Related Article 5:\n",
      "Title: Article 5\n",
      "Content: text:\n",
      "\n",
      "Document Chunk 9:\n",
      " article provides insights:\n",
      "\n",
      "Related Article 5:\n",
      "Title: Article 5\n",
      "Content: text: This is a list of mathematics-based methods.\n",
      "\n",
      "Adams' method (differential equations)\n",
      "Akra–Bazzi method (asymptotic analysis)\n",
      "Bisection method (root finding)\n",
      "Brent's method (root finding)\n",
      "Condorcet method (voting systems)\n",
      "Coombs' method (voting systems)\n",
      "Copeland's method (voting systems)\n",
      "Crank–Nicolson method (numerical analysis)\n",
      "D'Hondt method (voting systems)\n",
      "D21 – Janeček method (voting system)\n",
      "Discrete element method (numerical analysis)\n",
      "Domain decomposition method (numerical analysis)\n",
      "Epidemiological methods\n",
      "Euler's forward method\n",
      "Explicit and implicit methods (numerical analysis)\n",
      "Finite difference method (numerical analysis)\n",
      "Finite element method (numerical analysis)\n",
      "Finite volume method (numerical analysis)\n",
      "Highest averages method (voting systems)\n",
      "Method of exhaustion\n",
      "Method of infinite descent (number theory)\n",
      "Information bottleneck method\n",
      "Inverse chain rule method (calculus)\n",
      "Inverse transform sampling method (probability)\n",
      "Iterative method (numerical analysis)\n",
      "Jacobi method (linear algebra)\n",
      "Largest remainder method (voting systems)\n",
      "Level-set method\n",
      "Linear combination of atomic orbitals molecular orbital method (molecular orbitals)\n",
      "Method of characteristics\n",
      "Least squares method (optimization, statistics)\n",
      "Maximum likelihood method (statistics)\n",
      "Method of complements (arithmetic)\n",
      "Method of moving frames (differential geometry)\n",
      "Method of successive substitution (number theory)\n",
      "Monte Carlo method (computational physics, simulation)\n",
      "Newton's method (numerical analysis)\n",
      "Pemdas method (order of operation)\n",
      "Perturbation methods (functional analysis, quantum theory)\n",
      "Probabilistic method (combinatorics)\n",
      "Romberg's method (numerical analysis)\n",
      "Runge–Kutta method (numerical analysis)\n",
      "Sainte-Laguë method (voting systems)\n",
      "Schulze method (voting systems)\n",
      "Sequential Monte Carlo method\n",
      "Simplex method\n",
      "Spectral method (numerical analysis)\n",
      "Variational methods (mathematical analysis, differential\n",
      "\n",
      "Document Chunk 10:\n",
      "Spectral method (numerical analysis)\n",
      "Variational methods (mathematical analysis, differential equations)\n",
      "Welch's method\n",
      "\n",
      "See also\n",
      " Automatic basis function construction\n",
      " List of graphical methods\n",
      " Scientific method\n",
      "\n",
      "Methods\n",
      "Scientific method | url: https://en.wikipedia.org/wiki/List%20of%20mathematics-based%20methods | title: List of mathematics-based methods\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "filename = \"final_document.txt\"\n",
    "text_data = load_text_file(filename)\n",
    "\n",
    "raw_documents = [Document(page_content=text_data)]\n",
    "\n",
    "# Initialize the TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=20)\n",
    "\n",
    "# Split the Document object into smaller chunks\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# Display the split documents\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"Document Chunk {i}:\\n{doc.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Structure without graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of vectorstore: <class 'langchain_community.vectorstores.faiss.FAISS'>\n",
      "Number of documents: 10\n",
      "\n",
      "FAISS Index type: <class 'faiss.swigfaiss_avx2.IndexFlatL2'>\n",
      "FAISS Index dimension: 1536\n",
      "Total number of vectors: 10\n",
      "\n",
      "Example document IDs:\n",
      "Index 0: Document ID ab929b79-2688-4ffa-9515-de0dbbca7863\n",
      "Index 1: Document ID a6cba1f6-c64e-4577-9595-34142793a182\n",
      "Index 2: Document ID 81872651-e924-4567-b0b8-b66861de37a1\n",
      "Index 3: Document ID 1b1cb805-1a6e-4791-9754-40faf96ed201\n",
      "Index 4: Document ID fea3289c-eb8a-4ec3-be29-8416f0acad41\n",
      "Index 5: Document ID 2dd30020-91fd-4c30-a182-f09e348a9112\n",
      "Index 6: Document ID ad1dd5b9-7caf-453b-82c5-bd490ee2aa6c\n",
      "Index 7: Document ID 25e9b27e-e543-4544-9c74-87e82c9d834e\n",
      "Index 8: Document ID 8e90d7c1-6bd3-4818-a1af-5a675749405e\n",
      "Index 9: Document ID 6665f9c3-3319-486d-a867-734f21f07e2b\n",
      "The last two vector embeddings stored in vectorstore:\n",
      "\n",
      "[[ 0.00092509  0.0529552  -0.0011348  ... -0.01423323 -0.0100052\n",
      "  -0.04031847]\n",
      " [ 0.00058459  0.01741397  0.01649816 ... -0.0233129   0.00279964\n",
      "  -0.01309752]]\n"
     ]
    }
   ],
   "source": [
    "# Print basic information\n",
    "print(f\"Type of vectorstore: {type(vectorstore)}\")\n",
    "print(f\"Number of documents: {len(vectorstore.index_to_docstore_id)}\")\n",
    "\n",
    "# Print information about the underlying FAISS index\n",
    "faiss_index = vectorstore.index\n",
    "print(f\"\\nFAISS Index type: {type(faiss_index)}\")\n",
    "print(f\"FAISS Index dimension: {faiss_index.d}\")\n",
    "print(f\"Total number of vectors: {faiss_index.ntotal}\")\n",
    "\n",
    "\n",
    "# Print some example document IDs\n",
    "print(\"\\nExample document IDs:\")\n",
    "for i, doc_id in list(vectorstore.index_to_docstore_id.items())[:len(vectorstore.index_to_docstore_id)]:\n",
    "    print(f\"Index {i}: Document ID {doc_id}\")\n",
    "\n",
    "\n",
    "print('The last two vector embeddings stored in vectorstore:\\n')\n",
    "vectors = vectorstore.index.reconstruct_n(len(vectorstore.index_to_docstore_id)-2, 2)\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstration of RAG response:\n",
      "\n",
      "Creating a study plan for the \"Data-Driven Marketing\" course involves breaking\n",
      "down the course content into manageable sections, allocating time for each\n",
      "topic, and incorporating review sessions. Here's a suggested study plan based on\n",
      "the course overview and related topics. This plan assumes a 12-week course\n",
      "duration, which can be adjusted based on your actual course timeline.  ### Week\n",
      "1-2: Introduction to Data-Driven Marketing - **Objective:** Understand the\n",
      "basics of data-driven marketing, its importance, and how it differs from\n",
      "traditional marketing approaches. - **Activities:**   - Read course materials on\n",
      "the strategic value of marketing initiatives.   - Watch introductory videos on\n",
      "data-driven marketing.   - Participate in forum discussions about the role of\n",
      "data in marketing decisions.  ### Week 3-4: Marketing Performance Management &\n",
      "Metrics - **Objective:** Learn how to measure and manage marketing performance\n",
      "using various metrics. - **Activities:**   - Study different marketing metrics\n",
      "and their applications.   - Complete exercises on calculating and interpreting\n",
      "these metrics.   - Case study analysis on successful marketing campaigns and the\n",
      "metrics used.  ### Week 5-6: Data Management for Marketing - **Objective:** Gain\n",
      "skills in managing marketing data, including collection, storage, and analysis.\n",
      "- **Activities:**   - Learn about data management tools and techniques.   -\n",
      "Hands-on project: Collect and prepare a dataset for analysis.   - Explore case\n",
      "studies on data management in marketing.  ### Week 7-8: Market Response &\n",
      "Customer Segmentation - **Objective:** Understand how to analyze market response\n",
      "and segment the market based on customer data. - **Activities:**   - Study\n",
      "models of market response and diffusion.   - Project: Segment a market using\n",
      "real-world data.   - Review articles on market and customer segmentation\n",
      "strategies.  ### Week 9-10: Analytic Marketing & Digital Media Analytics -\n",
      "**Objective:** Dive into analytic marketing techniques and digital media\n",
      "marketing analytics. - **Activities:**   - Learn about tools and methods for\n",
      "analytic marketing.   - Analyze digital marketing campaigns using analytics\n",
      "tools.   - Group project: Design a data-driven digital marketing campaign.  ###\n",
      "Week 11-12: Leveraging Data for Marketing Decisions - **Objective:** Apply\n",
      "everything learned to make informed marketing decisions using data. -\n",
      "**Activities:**   - Develop a comprehensive marketing plan for a hypothetical\n",
      "product/service using data-driven insights.   - Peer review of marketing plans.\n",
      "- Final exam preparation: Review all course materials, focusing on areas of\n",
      "difficulty.  ### Weekly Routine: - **Mondays & Tuesdays:** Focus on reading\n",
      "materials and watching lecture videos. - **Wednesdays:** Participate in\n",
      "discussions or group projects. - **Thursdays & Fridays:** Work on exercises,\n",
      "case studies, or projects. - **Weekends:** Review the week's materials and\n",
      "prepare for upcoming topics.  ### Additional Tips: - **Regular Reviews:**\n",
      "Schedule weekly review sessions to go over notes and consolidate your learning.\n",
      "- **Stay Engaged:** Participate actively in forums and group discussions to\n",
      "deepen your understanding. - **Seek Feedback:** Regularly seek feedback on your\n",
      "projects and assignments from peers and instructors. - **Apply Learning:** Try\n",
      "to apply what you learn to real-world scenarios, even if hypothetically, to\n",
      "better grasp the concepts.  Adjust this plan as needed based on your pace of\n",
      "learning and any updates to the course schedule. Remember, the key to success in\n",
      "any course is consistent study, active participation, and the application of\n",
      "concepts to practical scenarios.\n"
     ]
    }
   ],
   "source": [
    "print('Demonstration of RAG response:\\n')\n",
    "#question_step3a = 'Who teaches R in the NUS course about business analytics, what else is taught and how are students graded?'\n",
    "question_step3a = 'Can you help me make a study plan for the course?'\n",
    "\n",
    "response = qa_chain.run(question_step3a)\n",
    "wrapped_response = textwrap.fill(response, width=80)\n",
    "print(wrapped_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see that the response is refined based on the documents, it is more related to the documents now. However, we notice that the wiki data is not fully utilized, thus we intend to explore more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_documents = llm_transformer.convert_to_graph_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of documents:10\n",
      "count of nodes in the first document chunk:10\n",
      "count of relationships in the first document chunk:3\n"
     ]
    }
   ],
   "source": [
    "print(f\"count of documents:{len(graph_documents)}\")\n",
    "print(f\"count of nodes in the first document chunk:{len(graph_documents[3].nodes)}\")\n",
    "print(f\"count of relationships in the first document chunk:{len(graph_documents[3].relationships)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As shown below, each of the document is split into nodes and relationships:\n",
      "\n",
      "Node ID: Status Epilepticus\n",
      "Node Type: Condition\n",
      "Node Properties: {}\n",
      "--------------------------------------------------\n",
      "Node ID: Tonic-Clonic Seizures\n",
      "Node Type: Condition\n",
      "Node Properties: {}\n",
      "--------------------------------------------------\n",
      "Node ID: Engel Classification System\n",
      "Node Type: System\n",
      "Node Properties: {}\n",
      "--------------------------------------------------\n",
      "Node ID: International League Against Epilepsy\n",
      "Node Type: Organization\n",
      "Node Properties: {}\n",
      "--------------------------------------------------\n",
      "Node ID: Ilae Rating Scale\n",
      "Node Type: System\n",
      "Node Properties: {}\n",
      "--------------------------------------------------\n",
      "Node ID: Unsupervised Learning\n",
      "Node Type: Concept\n",
      "Node Properties: {}\n",
      "--------------------------------------------------\n",
      "Node ID: Medical Terminology\n",
      "Node Type: Concept\n",
      "Node Properties: {}\n",
      "--------------------------------------------------\n",
      "Node ID: Neurological Disorders\n",
      "Node Type: Concept\n",
      "Node Properties: {}\n",
      "--------------------------------------------------\n",
      "Node ID: Neurology\n",
      "Node Type: Concept\n",
      "Node Properties: {}\n",
      "--------------------------------------------------\n",
      "Node ID: Epilepsy\n",
      "Node Type: Condition\n",
      "Node Properties: {}\n",
      "--------------------------------------------------\n",
      "Relationship from: Status Epilepticus (Type: Condition)\n",
      "  to: Tonic-Clonic Seizures (Type: Condition)\n",
      "Relationship Type: RELATED_TO\n",
      "Relationship Properties: {}\n",
      "--------------------------------------------------\n",
      "Relationship from: Engel Classification System (Type: System)\n",
      "  to: International League Against Epilepsy (Type: Organization)\n",
      "Relationship Type: EVALUATED_BY\n",
      "Relationship Properties: {}\n",
      "--------------------------------------------------\n",
      "Relationship from: Ilae Rating Scale (Type: System)\n",
      "  to: International League Against Epilepsy (Type: Organization)\n",
      "Relationship Type: DEVELOPED_BY\n",
      "Relationship Properties: {}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"As shown below, each of the document is split into nodes and relationships:\\n\")\n",
    "\n",
    "# Iterate through each item in graph_documents\n",
    "for item in graph_documents[3].nodes:\n",
    "    # Print details of the Node\n",
    "    print(f\"Node ID: {item.id}\")\n",
    "    print(f\"Node Type: {item.type}\")\n",
    "    print(f\"Node Properties: {item.properties}\")\n",
    "    print(\"-\" * 50)  # Separator for clarity\n",
    "\n",
    "for item in graph_documents[3].relationships:\n",
    "    # Print details of the relationships\n",
    "    print(f\"Relationship from: {item.source.id} (Type: {item.source.type})\")\n",
    "    print(f\"  to: {item.target.id} (Type: {item.target.type})\")\n",
    "    print(f\"Relationship Type: {item.type}\")\n",
    "    print(f\"Relationship Properties: {item.properties}\")\n",
    "    print(\"-\" * 50)  # Separator for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131\n"
     ]
    }
   ],
   "source": [
    "check_query = \"MATCH (n) RETURN count(n) AS node_count\"\n",
    "result = graph.query(check_query)\n",
    "for record in result:\n",
    "    print(record[\"node_count\"])  # Should print 0 if the database is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To create a new database, you can use Cypher query to delete all nodes and relationships\n",
    "clear_db_query = \"\"\"\n",
    "MATCH (n)\n",
    "DETACH DELETE n\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to clear the database\n",
    "graph.query(clear_db_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    # Ensures that each entity in graph_documents is labeled with its base entity type\n",
    "    baseEntityLabel=True,\n",
    "    # Indicate that the source information (like the original document or context) should be included in the graph nodes or edges.\n",
    "    include_source=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed3ebefffc64f41866d5c71c7172721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='500px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "default_cypher = \"MATCH (s)-[r]->(t) WHERE toLower(s.id) CONTAINS 'data' OR toLower(t.id) CONTAINS 'data' RETURN s, r, t\"\n",
    "# You can try other query\n",
    "# default_cypher = \"MATCH (s)-[r:IDENTIFY]->(t) RETURN s,r,t LIMIT 50\"\n",
    "\n",
    "# Function to display graph structure\n",
    "def showGraph(cypher: str = default_cypher):\n",
    "    # Create a neo4j session to run queries\n",
    "    driver = GraphDatabase.driver(\n",
    "        uri = os.environ[\"NEO4J_URI\"],\n",
    "        auth = (os.environ[\"NEO4J_USERNAME\"],\n",
    "                os.environ[\"NEO4J_PASSWORD\"]))\n",
    "    session = driver.session()\n",
    "    widget = GraphWidget(graph = session.run(cypher).graph())\n",
    "    widget.node_label_mapping = 'id'\n",
    "    return widget\n",
    "\n",
    "showGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d315e4bfc25643c2b74944a441a0eaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='800px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "showGraph(\"MATCH p=(d:Document)-[]->() RETURN p LIMIT 25 UNION MATCH p=()-[]->(d:Document) RETURN p;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    # Uses a model from OpenAI that converts text into vector embeddings which are used for vector-based search\n",
    "    OpenAIEmbeddings(),\n",
    "    # Search for similar words using a hybrid approach, combining both keyword-based and vector-based searches.\n",
    "    search_type=\"hybrid\",\n",
    "    # Only nodes with the Document label will be indexed\n",
    "    node_label=\"Document\",\n",
    "    # Within the node, we will return the 'text' property\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of the output of similarity search:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " there is no exactly matching strings in the result\n",
      "text: Course Overview:\n",
      "Title: Data-Driven Marketing | Description: In today’s environment, marketing or business analysts require tools and techniques to both quantify the strategic value of marketing initiatives, and to maximize marketing campaign performance. This course aims to teach students concepts, methods and tools to demonstrate the return on investment (ROI) of marketing activities and to leverage on data and marketing analytics to make better and more informed marketing decisions. Course topics covered include marketing performance management, marketing metrics, data management, market response and diffusion models, market and customer segmentation models, analytic marketing and value driven segmentation, digital media marketing analytics, etc. Students will have access to | Subject: Computer Science\n",
      "\n",
      "Related Article 1:\n",
      "Title: Article 1\n",
      "Content: text: User behavior analytics (UBA) is a cybersecurity process about detection of insider threats, targeted attacks, and financial fraud that tracks a system's users. UBA looks at patterns of human behavior, and then analyzes them to detect anomalies that indicate potential threats. Big data platforms like Apache Hadoop are increasing UBA functionality by allowing them to analyze petabytes worth of data to detect insider threats and advanced persistent threats.\n",
      "\n",
      "Purpose \n",
      "UBA's purpose, according to Johna Till Johnson of Nemertes Research, is that \"Security systems provide so much information that it's tough to uncover information that truly indicates a potential for real attack. Analytics tools help make sense of the vast amount of data that SIEM, IDS/IPS, system logs, and other tools gather. UBA tools use a specialized type of security analytics that focuses on the behavior of systems and the people using them. UBA technology first evolved in the field of marketing, to help companies understand and predict consumer-buying patterns. But as it turns out, UBA can be extraordinarily useful in the security context too.\"\n",
      "\n",
      "See also\n",
      " Behavioral analytics\n",
      " Network behavior anomaly detection\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      "\n",
      "ABC's Of UBA\n",
      "\n",
      "Software | url: https://en.wikipedia.org/wiki/User%20behavior%20analytics | title: User behavior analytics\n",
      "\n",
      "\n",
      "In addition, the following article provides insights:\n",
      "\n",
      "Related Article 2:\n",
      "Title: Article 2\n",
      "Content: text: Software installed in medical devices is assessed for health and safety issues according to international standards.\n",
      "\n",
      "Safety classes \n",
      "Software classification is based on the potential for hazard(s) that could cause injury to the user or patient.\n",
      "\n",
      "Per [[I\n",
      "\n",
      " there is no exactly matching strings in the result\n",
      "text: Spectral method (numerical analysis)\n",
      "Variational methods (mathematical analysis, differential equations)\n",
      "Welch's method\n",
      "\n",
      "See also\n",
      " Automatic basis function construction\n",
      " List of graphical methods\n",
      " Scientific method\n",
      "\n",
      "Methods\n",
      "Scientific method | url: https://en.wikipedia.org/wiki/List%20of%20mathematics-based%20methods | title: List of mathematics-based methods\n",
      "\n",
      "\n",
      " there is no exactly matching strings in the result\n",
      "text:  in a year as rarely occurring. The worthwhileness of the operation is ambiguous because worth can be interpreted differently by various patients and healthcare professionals. Keeping those caveats in mind, most neurologists and neurosurgeons who specialize in epilepsy would most likely agree, as would many epileptics and even laypeople, that any seizure that leads to a period of status epilepticus (seizure activity, especially of the tonic-clonic, or grand mal, type, for longer than about five to ten minutes, or more- some now say it should be as little as two- without an intervening return to normal, or any repeat seizures without a return to consciousness) is a medical emergency, objectively a major problem, and cannot be considered a satisfactory outcome (unless perhaps if the person had a fatal or very severe form of a neurodegenerative syndrome or other disease where such severe repeat seizures are not unusual, and there are a number of these diseases; even then, such an outcome is usually still not a cure, just an amelioration of a fatal condition or a very disabling condition). Continuing to have to endure a large number of tonic-clonic seizures (grand mal seizures) over a period of days, months, or even over the course of a year or two, would make it impossible to drive and very hard to hold a job away from home entailing much stress, and would pose limits on one's abilities to safely carry out the activities of daily living without at least some monitoring or assistance.\n",
      "\n",
      "The Engel classification system has been thought of as a cross-sectional grading system by medical professionals because it does not account for long term changes in patients.  It has been proposed that it would be more beneficial to reevaluate patients on an annual basis, and the International League Against Epilepsy (ILAE) devised a separate rating scale in 2001 that reevaluates patients on every annual anniversary of their surgery. The ILAE also developed their system in hopes of avoiding many of the subjective components found in the Engel system.\n",
      "\n",
      "References\n",
      "\n",
      "Medical terminology\n",
      "Neurological disorders\n",
      "Neurology\n",
      "Epilepsy | url: https://en.wikipedia.org/wiki/Engel%20classification | title: Engel classification\n",
      "\n",
      "\n",
      "In addition, the following article provides insights:\n",
      "\n",
      "Related Article 4:\n",
      "Title: Article 4\n",
      "Content: text: Unsupervised learning  is a type of algorithm that learns patterns from untagged data. The hope is that through\n",
      "\n",
      " there is no exactly matching strings in the result\n",
      "text:  for hazard(s) that could cause injury to the user or patient.\n",
      "\n",
      "Per [[IEC 62304|IEC 62304:2006] + A1:2015], the software can be divided into three separate classes:\n",
      " The SOFTWARE SYSTEM is software safety class A if:\n",
      "the SOFTWARE SYSTEM cannot contribute to a HAZARDOUS SITUATION; or\n",
      "the SOFTWARE SYSTEM can contribute to a HAZARDOUS SITUATION which does not result in unacceptable RISK after consideration of RISK CONTROL measures external to the SOFTWARE SYSTEM.\n",
      "The SOFTWARE SYSTEM is software safety class B if:\n",
      "the SOFTWARE SYSTEM can contribute to a HAZARDOUS SITUATION which results in unacceptable RISK after consideration of RISK CONTROL measures external to the SOFTWARE SYSTEM and the resulting possible HARM is non-SERIOUS INJURY.\n",
      "The SOFTWARE SYSTEM is software safety class C if:\n",
      "the SOFTWARE SYSTEM can contribute to a HAZARDOUS SITUATION which results in unacceptable RISK after consideration of RISK CONTROL measures external to the SOFTWARE SYSTEM and the resulting possible HARM is death or SERIOUS INJURY“\n",
      "\n",
      "Serious injury \n",
      "For the purpose of this classification, serious injury is defined as injury or illness that directly or indirectly is life threatening; results in permanent impairment of a body function or permanent damage to a body structure; or necessitates medical or surgical intervention to prevent permanent impairment of a body function or permanent damage to a body structure.\n",
      "\n",
      "References\n",
      "\n",
      "Software\n",
      "Occupational safety and health | url: https://en.wikipedia.org/wiki/Software%20safety%20classification | title: Software safety classification\n",
      "\n",
      "\n",
      "In addition, the following article provides insights:\n",
      "\n",
      "Related Article 3:\n",
      "Title: Article 3\n",
      "Content: text: To classify postoperative outcomes for epilepsy surgery, Jerome Engel proposed the following scheme, the Engel Epilepsy Surgery Outcome Scale, which has become the de facto standard when reporting results in the medical literature:\n",
      " Class I: Free of disabling seizures\n",
      " Class II: Rare disabling seizures (\"almost seizure-free\")\n",
      " Class III: Worthwhile improvement\n",
      " Class IV: No worthwhile improvement\n",
      "\n",
      "History\n",
      "\n",
      "Surgery for epilepsy patients has been used for over a century, but due to technological restrictions and insufficient knowledge of brain surgery, this treatment approach was relatively rare until the 1980s and 90s.  Prior to the 1980s, no classification system existed due to the lack of operations performed up\n"
     ]
    }
   ],
   "source": [
    "print('Example of the output of similarity search:\\n')\n",
    "# By default the the method will return the top 4 most similar results.\n",
    "# To tune this, we can add in a new parameter, k = number of results, in the similarity_search function.\n",
    "\n",
    "def display_matching_strings(results, query_string):\n",
    "  \"\"\"Displays page_content only if it contains the query_string from the top 4 search results.\"\"\"\n",
    "\n",
    "  for doc in results[:4]:\n",
    "      if query_string in doc.page_content:\n",
    "          print(\"\\n this is matching result: \" + doc.page_content)\n",
    "      else:\n",
    "          print(\"\\n there is no exactly matching strings in the result\" + doc.page_content)\n",
    "\n",
    "# The similarity_search method is used to retrieve documents or nodes based on their vector similarity to a given query.\n",
    "results = vector_index.similarity_search('Justification', k=4)\n",
    "# Please note that as we search node labeled as \"Document\",\n",
    "# the retrieved results could be very tedious as they are the text relevant to the query_string)\n",
    "display_matching_strings(results, 'Justification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "\n",
    "    # This line structures the output of the LLM to give a List of names.\n",
    "    names: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"All the course knowledge, teaching material, deliverable, expectation, level and assessment entities \"\n",
    "        \"appear in the text\",\n",
    "    )\n",
    "\n",
    "# Each tuple represents a message with a specific role and content that helps define how different messages should be strucutured\n",
    "# and formatted when interacting with the llm.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are tasked with extracting specific entities from the text. Focus on course knowledge, teaching material, deliverable, expectation, level and assessment entities\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Use the given format to extract information from the following \"\n",
    "            \"input: {question}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine the prompt template (prompt) with the language model that specifies that the output should be structured in a particular way, specifically to extract entitites.\n",
    "entity_chain = prompt | llm.with_structured_output(Entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstration that the entity chain can now extract the entities from the users query:\n",
      "\n",
      "['data driven marketing', 'study']\n"
     ]
    }
   ],
   "source": [
    "question_step3b2_2 = 'What is data driven marketing and how should i study it'\n",
    "\n",
    "print('Demonstration that the entity chain can now extract the entities from the users query:\\n')\n",
    "print(entity_chain.invoke({\"question\": question_step3b2_2}).names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query(\n",
    "    \"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_full_text_query(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a full-text search query for a given input string.\n",
    "\n",
    "    This function constructs a query string suitable for a full-text search.\n",
    "    It processes the input string by splitting it into words and appending a\n",
    "    similarity threshold (~2 changed characters) to each word, then combines\n",
    "    them using the AND operator. Useful for mapping entities from user questions\n",
    "    to database values, and allows for some misspelings.\n",
    "    \"\"\"\n",
    "    full_text_query = \"\"\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "    for word in words[:-1]:\n",
    "        full_text_query += f\" {word}~2 AND\"\n",
    "    full_text_query += f\" {words[-1]}~2\"\n",
    "    return full_text_query.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_lucene_chars(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove special characters that are not allowed in Lucene queries.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', input)\n",
    "\n",
    "def generate_full_text_query(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a full-text search query for a given input string.\n",
    "\n",
    "    This function constructs a query string suitable for a full-text search.\n",
    "    It processes the input string by splitting it into words and appending a\n",
    "    similarity threshold (~2 changed characters) to each word, then combines\n",
    "    them using the AND operator. Useful for mapping entities from user questions\n",
    "    to database values, and allows for some misspellings.\n",
    "    \"\"\"\n",
    "    full_text_query = \"\"\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "\n",
    "    if not words:\n",
    "        return \"\"\n",
    "\n",
    "    for word in words[:-1]:\n",
    "        full_text_query += f\" {word}~2 AND\"\n",
    "    full_text_query += f\" {words[-1]}~2\"\n",
    "\n",
    "    return full_text_query.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structured_retriever(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Collects the neighborhood of entities mentioned\n",
    "    in the question.\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    entities = entity_chain.invoke({\"question\": question})\n",
    "\n",
    "    for entity in entities.names:\n",
    "        # This Neo4j Cypher query performs a full-text search on nodes that have the required label, retrieving the top two matches\n",
    "        # based on the search term provided. After this, the query then looks for relationships that point to or from this entity,\n",
    "        # excluding relationships of type 'MENTIONS'.\n",
    "        response = graph.query(\n",
    "            \"\"\"\n",
    "            CALL db.index.fulltext.queryNodes('entity', $query, {limit: 2})\n",
    "            YIELD node, score\n",
    "            WITH node\n",
    "            MATCH (node)-[r]->(neighbor)\n",
    "            WHERE type(r) <> 'MENTIONS'\n",
    "            RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "            UNION ALL\n",
    "            MATCH (neighbor)-[r]->(node)\n",
    "            WHERE type(r) <> 'MENTIONS'\n",
    "            RETURN neighbor.id + ' - ' + type(r) + ' -> ' + node.id AS output\n",
    "            LIMIT 50\n",
    "            \"\"\",\n",
    "            {\"query\": generate_full_text_query(entity)},\n",
    "        )\n",
    "\n",
    "        # Append results\n",
    "        result += \"\\n\".join([el['output'] for el in response]) + \"\\n\"\n",
    "\n",
    "    return result.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of the output of a structured retriever: \n",
      "\n",
      "Data-Driven Marketing - SUBJECT -> Computer Science\n",
      "Status Epilepticus - RELATED_TO -> Tonic-Clonic Seizures\n",
      "Engel Classification System - EVALUATED_BY -> International League Against Epilepsy\n",
      "Ilae Rating Scale - DEVELOPED_BY -> International League Against Epilepsy\n",
      "Unsupervised Learning - INCLUDES -> Neural Networks\n",
      "Unsupervised Learning - INCLUDES -> Probabilistic Methods\n",
      "Unsupervised Learning - INCLUDES -> Clustering\n",
      "Unsupervised Learning - INCLUDES -> Anomaly Detection\n",
      "Unsupervised Learning - INCLUDES -> Latent Variable Models\n",
      "Unsupervised Learning - EXCLUDES -> Backpropagation\n",
      "Unsupervised Learning - EMPLOYS -> Hopfield Learning Rule\n",
      "Unsupervised Learning - EMPLOYS -> Boltzmann Learning Rule\n",
      "Unsupervised Learning - EMPLOYS -> Contrastive Divergence\n",
      "Unsupervised Learning - EMPLOYS -> Wake Sleep\n",
      "Unsupervised Learning - EMPLOYS -> Variational Inference\n",
      "Unsupervised Learning - EMPLOYS -> Maximum Likelihood\n",
      "Unsupervised Learning - EMPLOYS -> Maximum A Posteriori\n",
      "Unsupervised Learning - EMPLOYS -> Gibbs Sampling\n",
      "Unsupervised Learning - APPLICATION_OF -> Density Estimation\n",
      "Unsupervised Learning - CONTRASTED_WITH -> Supervised Learning\n",
      "Learning - USES -> Mimicry\n",
      "Discriminative Tasks - USES -> Supervised Learning\n",
      "Generative Tasks - USES -> Unsupervised Learning\n",
      "Object Recognition - USES -> Unsupervised Learning\n",
      "Object Recognition - FAVORS -> Supervised Learning\n",
      "Image Recognition - STARTED_WITH -> Supervised Learning\n",
      "Image Recognition - MOVED_TO -> Unsupervised Learning\n",
      "Energy Function - MEASURE_OF -> Network'S Activation State\n",
      "Energy Function - ROLE_IN -> Boltzmann Machines\n",
      "Boltzmann Machines - ANALOGY_WITH -> Cost Function\n",
      "Ludwig Boltzmann - ANALYSIS_OF -> Boltzmann Constant\n",
      "Rbm Network - RELATION_WITH -> Activation Pattern\n",
      "Paul Smolensky - CALLS -> Harmony\n",
      "Hopfield Networks - INSPIRED_BY -> Ferromagnetism\n",
      "Hebbian Learning - RELATED_TO -> Pattern Recognition\n",
      "Hebbian Learning - RELATED_TO -> Experiential Learning\n",
      "Hebbian Learning - PRINCIPLE_OF -> Donald Hebb\n",
      "Hebbian Learning - FUNCTION_OF -> Action Potentials\n",
      "Self-Organizing Map (Som) - PART_OF -> Unsupervised Learning Algorithms\n",
      "Adaptive Resonance Theory (Art) - PART_OF -> Unsupervised Learning Algorithms\n",
      "Adaptive Resonance Theory (Art) - USED_FOR -> Automatic Target Recognition\n",
      "Adaptive Resonance Theory (Art) - USED_FOR -> Seismic Signal Processing\n",
      "Cluster Analysis - PART_OF -> Unsupervised Learning\n",
      "Latent Variable Models - LEARNED_BY -> Method Of Moments\n",
      "Method Of Moments - APPROACH_FOR -> Unsupervised Learning\n",
      "Method Of Moments - ESTIMATES -> Unknown Parameters\n",
      "Unknown Parameters - RELATED_TO -> Moments\n",
      "Moments - OF -> Random Variables\n",
      "Moments - ESTIMATED_FROM -> Samples\n",
      "First Order Moment - IS -> Mean Vector\n",
      "Second Order Moment - IS -> Covariance Matrix\n"
     ]
    }
   ],
   "source": [
    "question_step3b2_2 = 'what are the key knowledge behind data driven marketing?'\n",
    "print('Example of the output of a structured retriever: \\n')\n",
    "print(structured_retriever(question_step3b2_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to combine both structured and unstructred data defined above into a prompt to be fed to the LLM\n",
    "def retriever(question: str):\n",
    "    print(f\"Search query: {question}\")\n",
    "    structured_data = structured_retriever(question)\n",
    "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
    "    final_data = f\"\"\"Structured data:\n",
    "{structured_data}\n",
    "Unstructured data:\n",
    "{\"#Document \". join(unstructured_data)}\n",
    "    \"\"\"\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of the output of final retriever: \n",
      "\n",
      "Search query: what are the key knowledge behind data driven marketing?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured data:\n",
      "Status Epilepticus - RELATED_TO -> Tonic-Clonic Seizures\n",
      "Engel Classification System - EVALUATED_BY -> International League Against Epilepsy\n",
      "Ilae Rating Scale - DEVELOPED_BY -> International League Against Epilepsy\n",
      "Unsupervised Learning - INCLUDES -> Neural Networks\n",
      "Unsupervised Learning - INCLUDES -> Probabilistic Methods\n",
      "Unsupervised Learning - INCLUDES -> Clustering\n",
      "Unsupervised Learning - INCLUDES -> Anomaly Detection\n",
      "Unsupervised Learning - INCLUDES -> Latent Variable Models\n",
      "Unsupervised Learning - EXCLUDES -> Backpropagation\n",
      "Unsupervised Learning - EMPLOYS -> Hopfield Learning Rule\n",
      "Unsupervised Learning - EMPLOYS -> Boltzmann Learning Rule\n",
      "Unsupervised Learning - EMPLOYS -> Contrastive Divergence\n",
      "Unsupervised Learning - EMPLOYS -> Wake Sleep\n",
      "Unsupervised Learning - EMPLOYS -> Variational Inference\n",
      "Unsupervised Learning - EMPLOYS -> Maximum Likelihood\n",
      "Unsupervised Learning - EMPLOYS -> Maximum A Posteriori\n",
      "Unsupervised Learning - EMPLOYS -> Gibbs Sampling\n",
      "Unsupervised Learning - APPLICATION_OF -> Density Estimation\n",
      "Unsupervised Learning - CONTRASTED_WITH -> Supervised Learning\n",
      "Learning - USES -> Mimicry\n",
      "Discriminative Tasks - USES -> Supervised Learning\n",
      "Generative Tasks - USES -> Unsupervised Learning\n",
      "Object Recognition - USES -> Unsupervised Learning\n",
      "Object Recognition - FAVORS -> Supervised Learning\n",
      "Image Recognition - STARTED_WITH -> Supervised Learning\n",
      "Image Recognition - MOVED_TO -> Unsupervised Learning\n",
      "Energy Function - MEASURE_OF -> Network'S Activation State\n",
      "Energy Function - ROLE_IN -> Boltzmann Machines\n",
      "Boltzmann Machines - ANALOGY_WITH -> Cost Function\n",
      "Ludwig Boltzmann - ANALYSIS_OF -> Boltzmann Constant\n",
      "Rbm Network - RELATION_WITH -> Activation Pattern\n",
      "Paul Smolensky - CALLS -> Harmony\n",
      "Hopfield Networks - INSPIRED_BY -> Ferromagnetism\n",
      "Hebbian Learning - RELATED_TO -> Pattern Recognition\n",
      "Hebbian Learning - RELATED_TO -> Experiential Learning\n",
      "Hebbian Learning - PRINCIPLE_OF -> Donald Hebb\n",
      "Hebbian Learning - FUNCTION_OF -> Action Potentials\n",
      "Self-Organizing Map (Som) - PART_OF -> Unsupervised Learning Algorithms\n",
      "Adaptive Resonance Theory (Art) - PART_OF -> Unsupervised Learning Algorithms\n",
      "Adaptive Resonance Theory (Art) - USED_FOR -> Automatic Target Recognition\n",
      "Adaptive Resonance Theory (Art) - USED_FOR -> Seismic Signal Processing\n",
      "Cluster Analysis - PART_OF -> Unsupervised Learning\n",
      "Latent Variable Models - LEARNED_BY -> Method Of Moments\n",
      "Method Of Moments - APPROACH_FOR -> Unsupervised Learning\n",
      "Method Of Moments - ESTIMATES -> Unknown Parameters\n",
      "Unknown Parameters - RELATED_TO -> Moments\n",
      "Moments - OF -> Random Variables\n",
      "Moments - ESTIMATED_FROM -> Samples\n",
      "First Order Moment - IS -> Mean Vector\n",
      "Second Order Moment - IS -> Covariance Matrix\n",
      "Unstructured data:\n",
      "\n",
      "text: Course Overview:\n",
      "Title: Data-Driven Marketing | Description: In today’s environment, marketing or business analysts require tools and techniques to both quantify the strategic value of marketing initiatives, and to maximize marketing campaign performance. This course aims to teach students concepts, methods and tools to demonstrate the return on investment (ROI) of marketing activities and to leverage on data and marketing analytics to make better and more informed marketing decisions. Course topics covered include marketing performance management, marketing metrics, data management, market response and diffusion models, market and customer segmentation models, analytic marketing and value driven segmentation, digital media marketing analytics, etc. Students will have access to | Subject: Computer Science\n",
      "\n",
      "Related Article 1:\n",
      "Title: Article 1\n",
      "Content: text: User behavior analytics (UBA) is a cybersecurity process about detection of insider threats, targeted attacks, and financial fraud that tracks a system's users. UBA looks at patterns of human behavior, and then analyzes them to detect anomalies that indicate potential threats. Big data platforms like Apache Hadoop are increasing UBA functionality by allowing them to analyze petabytes worth of data to detect insider threats and advanced persistent threats.\n",
      "\n",
      "Purpose \n",
      "UBA's purpose, according to Johna Till Johnson of Nemertes Research, is that \"Security systems provide so much information that it's tough to uncover information that truly indicates a potential for real attack. Analytics tools help make sense of the vast amount of data that SIEM, IDS/IPS, system logs, and other tools gather. UBA tools use a specialized type of security analytics that focuses on the behavior of systems and the people using them. UBA technology first evolved in the field of marketing, to help companies understand and predict consumer-buying patterns. But as it turns out, UBA can be extraordinarily useful in the security context too.\"\n",
      "\n",
      "See also\n",
      " Behavioral analytics\n",
      " Network behavior anomaly detection\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      "\n",
      "ABC's Of UBA\n",
      "\n",
      "Software | url: https://en.wikipedia.org/wiki/User%20behavior%20analytics | title: User behavior analytics\n",
      "\n",
      "\n",
      "In addition, the following article provides insights:\n",
      "\n",
      "Related Article 2:\n",
      "Title: Article 2\n",
      "Content: text: Software installed in medical devices is assessed for health and safety issues according to international standards.\n",
      "\n",
      "Safety classes \n",
      "Software classification is based on the potential for hazard(s) that could cause injury to the user or patient.\n",
      "\n",
      "Per [[I#Document \n",
      "text:  STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.\n",
      "\n",
      "Among neural network models, the self-organizing map (SOM) and adaptive resonance theory (ART) are commonly used in unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing.\n",
      "\n",
      "Probabilistic methods \n",
      "Two of the main methods used in unsupervised learning are principal component and cluster analysis. Cluster analysis is used in unsupervised learning to group, or segment, datasets with shared attributes in order to extrapolate algorithmic relationships. Cluster analysis is a branch of machine learning that groups the data that has not been labelled, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. This approach helps detect anomalous data points that do not fit into either group.\n",
      "\n",
      "A central application of unsupervised learning is in the field of density estimation in statistics, though unsupervised learning encompasses many other domains involving summarizing and explaining data features. It can be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution  conditioned on the label  of input data; unsupervised learning intends to infer an a priori probability distribution .\n",
      "\n",
      "Approaches \n",
      "Some of the most common algorithms used in unsupervised learning include: (1) Clustering, (2) Anomaly detection, (3) Approaches for learning latent variable models. Each approach uses several methods as follows:\n",
      "\n",
      " Clustering methods include: hierarchical clustering, k-means, mixture models, DBSCAN, and OPTICS algorithm\n",
      " Anomaly detection methods include: Local Outlier Factor, and Isolation Forest\n",
      " Approaches for learning latent variable models such as Expectation–maximization algorithm (EM), Method of moments, and Blind signal separation techniques (Principal component analysis, Independent component analysis, Non-negative matrix factorization, Singular value decomposition)\n",
      "\n",
      "Method of moments \n",
      "One of the statistical approaches#Document \n",
      "text:  learning  is a type of algorithm that learns patterns from untagged data. The hope is that through mimicry, which is an important mode of learning in people, the machine is forced to build a compact internal representation of its world and then generate imaginative content from it. In contrast to supervised learning where data is tagged by an expert, e.g. as a \"ball\" or \"fish\", unsupervised methods exhibit self-organization that captures patterns as probability densities  or a combination of neural feature preferences. The other levels in the supervision spectrum are reinforcement learning where the machine is given only a numerical performance score as guidance, and semi-supervised learning where a smaller portion of the data is tagged. Two broad methods in Unsupervised Learning are Neural Networks and Probabilistic Methods.\n",
      "\n",
      "Neural networks\n",
      "\n",
      "Tasks vs. Methods \n",
      "\n",
      "Neural network tasks are often categorized as discriminative (recognition) or generative (imagination).  Often but not always, discriminative tasks use supervised methods and generative tasks use unsupervised (see Venn diagram); however, the separation is very hazy.  For example, object recognition favors supervised learning but unsupervised learning can also cluster objects into groups.  Furthermore, as progress marches onward some tasks employ both methods, and some tasks swing from one to another.  For example, image recognition started off as heavily supervised, but became hybrid by employing unsupervised pre-training, and then moved towards supervision again with the advent of dropout, relu, and adaptive learning rates.\n",
      "\n",
      "Training \n",
      "During the learning phase, an unsupervised network tries to mimic the data it's given and uses the error in its mimicked output to correct itself (ie. correct its weights & biases). This resembles the mimicry behavior of children as they learn a language.  Sometimes the error is expressed as a low probability that the erroneous output occurs, or it might be express as an unstable high energy state in the network.\n",
      "\n",
      "In contrast to Supervised method's dominant use of Backpropagation, Unsupervised Learning also employ other methods  including:  Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations.  See the table below for more details.\n",
      "\n",
      "Energy \n",
      "An energy function is a#Document \n",
      "text: .  See the table below for more details.\n",
      "\n",
      "Energy \n",
      "An energy function is a macroscopic measure of a network's activation state.  In Boltzmann machines, it plays the role of the Cost function.  This analogy with physics is inspired by Ludwig Boltzmann's analysis of a gas' macroscopic energy from the microscopic probabilities of particle motion p  eE/kT, where k is the Boltzmann constant and T is temperature. In the RBM network the relation is p = e−E / Z, where p & E vary over every possible activation pattern and Z =  e -E(pattern). To be more precise, p(a) = e-E(a) / Z, where a is an activation pattern of all neurons (visible and hidden). Hence, early neural networks bear the name Boltzmann Machine.  Paul Smolensky calls -E the Harmony. A network seeks low energy which is high Harmony.\n",
      "\n",
      "Networks \n",
      "This table shows connection diagrams of various unsupervised networks, the details of which will be given in the section Comparison of Network.  Of the networks bearing people's names, only Hopfield worked directly with neural networks.  Boltzmann and Helmholtz lived before the invention of artificial neural networks, but they did inspire the analytical methods that were used.\n",
      "\n",
      "History\n",
      "\n",
      "Specific Networks \n",
      "\n",
      "Here, we highlight some characteristics of each networks. Ferromagnetism inspired Hopfield networks, Boltzmann machines, and RBMs. A neuron correspond to an iron domain with binary magnetic moments Up and Down, and neural connections correspond to the domain's influence on each other. Symmetric connections enables a global energy formulation. During inference the network updates each state using the standard activation step function. Symmetric weights guarantees convergence to a stable activation pattern.\n",
      "\n",
      "Comparison of Networks \n",
      "\n",
      "Hebbian Learning, ART, SOM\n",
      "The classical example of unsupervised learning in the study of neural networks is Donald Hebb's principle, that is, neurons that fire together wire together. In Hebbian learning, the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons. A similar version that modifies synaptic weights takes into account the time between the action potentials (spike-timing-dependent plasticity or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "question_step3b2_3 = 'what are the key knowledge behind data driven marketing?'\n",
    "print('Example of the output of final retriever: \\n')\n",
    "print(retriever(question_step3b2_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condense a chat history and follow-up question into a standalone question\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
    "in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "# Formats chat history to incorporate into a query for the LLM\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),  # Condense follow-up question and chat into a standalone_question\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatOpenAI(temperature=0)\n",
    "        | StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    RunnableLambda(lambda x : x[\"question\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_question = 'Can you help me make a study plan for the course?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unstructure retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def just_unstructured_retriever(question: str):\n",
    "    print(f\"Search query: {question}\")\n",
    "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
    "    final_data = f\"\"\"\n",
    "Unstructured data:\n",
    "{\"#Document \". join(unstructured_data)}\n",
    "    \"\"\"\n",
    "    return final_data\n",
    "\n",
    "# Prompt Augumentation: it instructs the model to answer a question using only the context provided.\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Use natural language and be concise.\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM Generation by running two operations in parallel: retrieve context and passthrough quesiton\n",
    "unstructured_chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": _search_query | just_unstructured_retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of the retrieval output fed to the LLM: \n",
      "\n",
      "Search query: Can you help me make a study plan for the course?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unstructured data:\n",
      "\n",
      "text:  STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.\n",
      "\n",
      "Among neural network models, the self-organizing map (SOM) and adaptive resonance theory (ART) are commonly used in unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing.\n",
      "\n",
      "Probabilistic methods \n",
      "Two of the main methods used in unsupervised learning are principal component and cluster analysis. Cluster analysis is used in unsupervised learning to group, or segment, datasets with shared attributes in order to extrapolate algorithmic relationships. Cluster analysis is a branch of machine learning that groups the data that has not been labelled, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. This approach helps detect anomalous data points that do not fit into either group.\n",
      "\n",
      "A central application of unsupervised learning is in the field of density estimation in statistics, though unsupervised learning encompasses many other domains involving summarizing and explaining data features. It can be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution  conditioned on the label  of input data; unsupervised learning intends to infer an a priori probability distribution .\n",
      "\n",
      "Approaches \n",
      "Some of the most common algorithms used in unsupervised learning include: (1) Clustering, (2) Anomaly detection, (3) Approaches for learning latent variable models. Each approach uses several methods as follows:\n",
      "\n",
      " Clustering methods include: hierarchical clustering, k-means, mixture models, DBSCAN, and OPTICS algorithm\n",
      " Anomaly detection methods include: Local Outlier Factor, and Isolation Forest\n",
      " Approaches for learning latent variable models such as Expectation–maximization algorithm (EM), Method of moments, and Blind signal separation techniques (Principal component analysis, Independent component analysis, Non-negative matrix factorization, Singular value decomposition)\n",
      "\n",
      "Method of moments \n",
      "One of the statistical approaches#Document \n",
      "text: Course Overview:\n",
      "Title: Data-Driven Marketing | Description: In today’s environment, marketing or business analysts require tools and techniques to both quantify the strategic value of marketing initiatives, and to maximize marketing campaign performance. This course aims to teach students concepts, methods and tools to demonstrate the return on investment (ROI) of marketing activities and to leverage on data and marketing analytics to make better and more informed marketing decisions. Course topics covered include marketing performance management, marketing metrics, data management, market response and diffusion models, market and customer segmentation models, analytic marketing and value driven segmentation, digital media marketing analytics, etc. Students will have access to | Subject: Computer Science\n",
      "\n",
      "Related Article 1:\n",
      "Title: Article 1\n",
      "Content: text: User behavior analytics (UBA) is a cybersecurity process about detection of insider threats, targeted attacks, and financial fraud that tracks a system's users. UBA looks at patterns of human behavior, and then analyzes them to detect anomalies that indicate potential threats. Big data platforms like Apache Hadoop are increasing UBA functionality by allowing them to analyze petabytes worth of data to detect insider threats and advanced persistent threats.\n",
      "\n",
      "Purpose \n",
      "UBA's purpose, according to Johna Till Johnson of Nemertes Research, is that \"Security systems provide so much information that it's tough to uncover information that truly indicates a potential for real attack. Analytics tools help make sense of the vast amount of data that SIEM, IDS/IPS, system logs, and other tools gather. UBA tools use a specialized type of security analytics that focuses on the behavior of systems and the people using them. UBA technology first evolved in the field of marketing, to help companies understand and predict consumer-buying patterns. But as it turns out, UBA can be extraordinarily useful in the security context too.\"\n",
      "\n",
      "See also\n",
      " Behavioral analytics\n",
      " Network behavior anomaly detection\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      "\n",
      "ABC's Of UBA\n",
      "\n",
      "Software | url: https://en.wikipedia.org/wiki/User%20behavior%20analytics | title: User behavior analytics\n",
      "\n",
      "\n",
      "In addition, the following article provides insights:\n",
      "\n",
      "Related Article 2:\n",
      "Title: Article 2\n",
      "Content: text: Software installed in medical devices is assessed for health and safety issues according to international standards.\n",
      "\n",
      "Safety classes \n",
      "Software classification is based on the potential for hazard(s) that could cause injury to the user or patient.\n",
      "\n",
      "Per [[I#Document \n",
      "text:  article provides insights:\n",
      "\n",
      "Related Article 5:\n",
      "Title: Article 5\n",
      "Content: text: This is a list of mathematics-based methods.\n",
      "\n",
      "Adams' method (differential equations)\n",
      "Akra–Bazzi method (asymptotic analysis)\n",
      "Bisection method (root finding)\n",
      "Brent's method (root finding)\n",
      "Condorcet method (voting systems)\n",
      "Coombs' method (voting systems)\n",
      "Copeland's method (voting systems)\n",
      "Crank–Nicolson method (numerical analysis)\n",
      "D'Hondt method (voting systems)\n",
      "D21 – Janeček method (voting system)\n",
      "Discrete element method (numerical analysis)\n",
      "Domain decomposition method (numerical analysis)\n",
      "Epidemiological methods\n",
      "Euler's forward method\n",
      "Explicit and implicit methods (numerical analysis)\n",
      "Finite difference method (numerical analysis)\n",
      "Finite element method (numerical analysis)\n",
      "Finite volume method (numerical analysis)\n",
      "Highest averages method (voting systems)\n",
      "Method of exhaustion\n",
      "Method of infinite descent (number theory)\n",
      "Information bottleneck method\n",
      "Inverse chain rule method (calculus)\n",
      "Inverse transform sampling method (probability)\n",
      "Iterative method (numerical analysis)\n",
      "Jacobi method (linear algebra)\n",
      "Largest remainder method (voting systems)\n",
      "Level-set method\n",
      "Linear combination of atomic orbitals molecular orbital method (molecular orbitals)\n",
      "Method of characteristics\n",
      "Least squares method (optimization, statistics)\n",
      "Maximum likelihood method (statistics)\n",
      "Method of complements (arithmetic)\n",
      "Method of moving frames (differential geometry)\n",
      "Method of successive substitution (number theory)\n",
      "Monte Carlo method (computational physics, simulation)\n",
      "Newton's method (numerical analysis)\n",
      "Pemdas method (order of operation)\n",
      "Perturbation methods (functional analysis, quantum theory)\n",
      "Probabilistic method (combinatorics)\n",
      "Romberg's method (numerical analysis)\n",
      "Runge–Kutta method (numerical analysis)\n",
      "Sainte-Laguë method (voting systems)\n",
      "Schulze method (voting systems)\n",
      "Sequential Monte Carlo method\n",
      "Simplex method\n",
      "Spectral method (numerical analysis)\n",
      "Variational methods (mathematical analysis, differential#Document \n",
      "text: Spectral method (numerical analysis)\n",
      "Variational methods (mathematical analysis, differential equations)\n",
      "Welch's method\n",
      "\n",
      "See also\n",
      " Automatic basis function construction\n",
      " List of graphical methods\n",
      " Scientific method\n",
      "\n",
      "Methods\n",
      "Scientific method | url: https://en.wikipedia.org/wiki/List%20of%20mathematics-based%20methods | title: List of mathematics-based methods\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print('Example of the retrieval output fed to the LLM: \\n')\n",
    "print(just_unstructured_retriever(demo_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unstructured retrieval model response: \n",
      "\n",
      "Search query: Can you help me make a study plan for the course?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's a concise study plan for your Data-Driven Marketing course:\n",
      "1. **Week 1-2: Foundations of Data-Driven Marketing**    - Focus on\n",
      "understanding the basic concepts and the strategic value of marketing\n",
      "initiatives.    - Read about marketing performance management and marketing\n",
      "metrics.  2. **Week 3-4: Data Management and Analysis**    - Dive into data\n",
      "management techniques, learning how to organize and interpret marketing data.\n",
      "- Practice with real datasets if possible, focusing on market response and\n",
      "diffusion models.  3. **Week 5-6: Market and Customer Segmentation**    - Study\n",
      "market and customer segmentation models.    - Apply these concepts to case\n",
      "studies or real-world marketing scenarios.  4. **Week 7-8: Analytic Marketing\n",
      "and Value-Driven Segmentation**    - Explore advanced topics in analytic\n",
      "marketing.    - Work on projects that require you to segment markets based on\n",
      "value-driven parameters.  5. **Week 9-10: Digital Media Marketing Analytics**\n",
      "- Learn about the tools and techniques used in digital media marketing\n",
      "analytics.    - Analyze digital marketing campaigns and their ROI.  6. **Week\n",
      "11-12: Application and Project Work**    - Apply the concepts learned to a\n",
      "comprehensive project, possibly focusing on a real-world problem.    - Utilize\n",
      "tools and techniques learned throughout the course to demonstrate ROI of\n",
      "marketing activities.  7. **Review and Exam Preparation**    - Review all course\n",
      "materials, focusing on areas of difficulty.    - Engage in group study sessions\n",
      "for collaborative learning and problem-solving.  Throughout the course: - Stay\n",
      "updated with related articles and current trends in data-driven marketing. -\n",
      "Regularly participate in discussions and practical exercises. - Utilize online\n",
      "resources and forums for additional learning and support.  This plan is designed\n",
      "to build your knowledge progressively, ensuring a deep understanding of data-\n",
      "driven marketing by the end of the course.\n"
     ]
    }
   ],
   "source": [
    "print('Unstructured retrieval model response: \\n')\n",
    "response = unstructured_chain.invoke({\"question\": demo_question})\n",
    "wrapped_response = textwrap.fill(response, width=80)\n",
    "print(wrapped_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def just_structured_retriever(question: str):\n",
    "    print(f\"Search query: {question}\")\n",
    "    structured_data = structured_retriever(question)\n",
    "   # unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
    "    final_data = f\"\"\"Structured data:\n",
    "{structured_data}\n",
    "    \"\"\"\n",
    "    return final_data\n",
    "\n",
    "# Prompt Augumentation: it instructs the model to answer a question using only the context provided.\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Use natural language and be concise.\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM Generation by running two operations in parallel: retrieve context and passthrough quesiton\n",
    "structured_chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": _search_query | just_structured_retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of the retrieval output fed to the LLM: \n",
      "\n",
      "Search query: Can you help me make a study plan for the course?\n",
      "Structured data:\n",
      "Status Epilepticus - RELATED_TO -> Tonic-Clonic Seizures\n",
      "Engel Classification System - EVALUATED_BY -> International League Against Epilepsy\n",
      "Ilae Rating Scale - DEVELOPED_BY -> International League Against Epilepsy\n",
      "Unsupervised Learning - INCLUDES -> Neural Networks\n",
      "Unsupervised Learning - INCLUDES -> Probabilistic Methods\n",
      "Unsupervised Learning - INCLUDES -> Clustering\n",
      "Unsupervised Learning - INCLUDES -> Anomaly Detection\n",
      "Unsupervised Learning - INCLUDES -> Latent Variable Models\n",
      "Unsupervised Learning - EXCLUDES -> Backpropagation\n",
      "Unsupervised Learning - EMPLOYS -> Hopfield Learning Rule\n",
      "Unsupervised Learning - EMPLOYS -> Boltzmann Learning Rule\n",
      "Unsupervised Learning - EMPLOYS -> Contrastive Divergence\n",
      "Unsupervised Learning - EMPLOYS -> Wake Sleep\n",
      "Unsupervised Learning - EMPLOYS -> Variational Inference\n",
      "Unsupervised Learning - EMPLOYS -> Maximum Likelihood\n",
      "Unsupervised Learning - EMPLOYS -> Maximum A Posteriori\n",
      "Unsupervised Learning - EMPLOYS -> Gibbs Sampling\n",
      "Unsupervised Learning - APPLICATION_OF -> Density Estimation\n",
      "Unsupervised Learning - CONTRASTED_WITH -> Supervised Learning\n",
      "Learning - USES -> Mimicry\n",
      "Discriminative Tasks - USES -> Supervised Learning\n",
      "Generative Tasks - USES -> Unsupervised Learning\n",
      "Object Recognition - USES -> Unsupervised Learning\n",
      "Object Recognition - FAVORS -> Supervised Learning\n",
      "Image Recognition - STARTED_WITH -> Supervised Learning\n",
      "Image Recognition - MOVED_TO -> Unsupervised Learning\n",
      "Energy Function - MEASURE_OF -> Network'S Activation State\n",
      "Energy Function - ROLE_IN -> Boltzmann Machines\n",
      "Boltzmann Machines - ANALOGY_WITH -> Cost Function\n",
      "Ludwig Boltzmann - ANALYSIS_OF -> Boltzmann Constant\n",
      "Rbm Network - RELATION_WITH -> Activation Pattern\n",
      "Paul Smolensky - CALLS -> Harmony\n",
      "Hopfield Networks - INSPIRED_BY -> Ferromagnetism\n",
      "Hebbian Learning - RELATED_TO -> Pattern Recognition\n",
      "Hebbian Learning - RELATED_TO -> Experiential Learning\n",
      "Hebbian Learning - PRINCIPLE_OF -> Donald Hebb\n",
      "Hebbian Learning - FUNCTION_OF -> Action Potentials\n",
      "Self-Organizing Map (Som) - PART_OF -> Unsupervised Learning Algorithms\n",
      "Adaptive Resonance Theory (Art) - PART_OF -> Unsupervised Learning Algorithms\n",
      "Adaptive Resonance Theory (Art) - USED_FOR -> Automatic Target Recognition\n",
      "Adaptive Resonance Theory (Art) - USED_FOR -> Seismic Signal Processing\n",
      "Cluster Analysis - PART_OF -> Unsupervised Learning\n",
      "Latent Variable Models - LEARNED_BY -> Method Of Moments\n",
      "Method Of Moments - APPROACH_FOR -> Unsupervised Learning\n",
      "Method Of Moments - ESTIMATES -> Unknown Parameters\n",
      "Unknown Parameters - RELATED_TO -> Moments\n",
      "Moments - OF -> Random Variables\n",
      "Moments - ESTIMATED_FROM -> Samples\n",
      "First Order Moment - IS -> Mean Vector\n",
      "Second Order Moment - IS -> Covariance Matrix\n",
      "Status Epilepticus - RELATED_TO -> Tonic-Clonic Seizures\n",
      "Engel Classification System - EVALUATED_BY -> International League Against Epilepsy\n",
      "Ilae Rating Scale - DEVELOPED_BY -> International League Against Epilepsy\n",
      "Unsupervised Learning - INCLUDES -> Neural Networks\n",
      "Unsupervised Learning - INCLUDES -> Probabilistic Methods\n",
      "Unsupervised Learning - INCLUDES -> Clustering\n",
      "Unsupervised Learning - INCLUDES -> Anomaly Detection\n",
      "Unsupervised Learning - INCLUDES -> Latent Variable Models\n",
      "Unsupervised Learning - EXCLUDES -> Backpropagation\n",
      "Unsupervised Learning - EMPLOYS -> Hopfield Learning Rule\n",
      "Unsupervised Learning - EMPLOYS -> Boltzmann Learning Rule\n",
      "Unsupervised Learning - EMPLOYS -> Contrastive Divergence\n",
      "Unsupervised Learning - EMPLOYS -> Wake Sleep\n",
      "Unsupervised Learning - EMPLOYS -> Variational Inference\n",
      "Unsupervised Learning - EMPLOYS -> Maximum Likelihood\n",
      "Unsupervised Learning - EMPLOYS -> Maximum A Posteriori\n",
      "Unsupervised Learning - EMPLOYS -> Gibbs Sampling\n",
      "Unsupervised Learning - APPLICATION_OF -> Density Estimation\n",
      "Unsupervised Learning - CONTRASTED_WITH -> Supervised Learning\n",
      "Learning - USES -> Mimicry\n",
      "Discriminative Tasks - USES -> Supervised Learning\n",
      "Generative Tasks - USES -> Unsupervised Learning\n",
      "Object Recognition - USES -> Unsupervised Learning\n",
      "Object Recognition - FAVORS -> Supervised Learning\n",
      "Image Recognition - STARTED_WITH -> Supervised Learning\n",
      "Image Recognition - MOVED_TO -> Unsupervised Learning\n",
      "Energy Function - MEASURE_OF -> Network'S Activation State\n",
      "Energy Function - ROLE_IN -> Boltzmann Machines\n",
      "Boltzmann Machines - ANALOGY_WITH -> Cost Function\n",
      "Ludwig Boltzmann - ANALYSIS_OF -> Boltzmann Constant\n",
      "Rbm Network - RELATION_WITH -> Activation Pattern\n",
      "Paul Smolensky - CALLS -> Harmony\n",
      "Hopfield Networks - INSPIRED_BY -> Ferromagnetism\n",
      "Hebbian Learning - RELATED_TO -> Pattern Recognition\n",
      "Hebbian Learning - RELATED_TO -> Experiential Learning\n",
      "Hebbian Learning - PRINCIPLE_OF -> Donald Hebb\n",
      "Hebbian Learning - FUNCTION_OF -> Action Potentials\n",
      "Self-Organizing Map (Som) - PART_OF -> Unsupervised Learning Algorithms\n",
      "Adaptive Resonance Theory (Art) - PART_OF -> Unsupervised Learning Algorithms\n",
      "Adaptive Resonance Theory (Art) - USED_FOR -> Automatic Target Recognition\n",
      "Adaptive Resonance Theory (Art) - USED_FOR -> Seismic Signal Processing\n",
      "Cluster Analysis - PART_OF -> Unsupervised Learning\n",
      "Latent Variable Models - LEARNED_BY -> Method Of Moments\n",
      "Method Of Moments - APPROACH_FOR -> Unsupervised Learning\n",
      "Method Of Moments - ESTIMATES -> Unknown Parameters\n",
      "Unknown Parameters - RELATED_TO -> Moments\n",
      "Moments - OF -> Random Variables\n",
      "Moments - ESTIMATED_FROM -> Samples\n",
      "First Order Moment - IS -> Mean Vector\n",
      "Second Order Moment - IS -> Covariance Matrix\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print('Example of the retrieval output fed to the LLM: \\n')\n",
    "print(just_structured_retriever(demo_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured retrieval model response: \n",
      "\n",
      "Search query: Can you help me make a study plan for the course?\n",
      "Certainly! Here's a concise study plan for your course based on the provided\n",
      "context:  1. **Introduction to Unsupervised Learning**:    - Understand the\n",
      "basics of unsupervised learning, including its definition and how it contrasts\n",
      "with supervised learning.    - Explore the types of problems unsupervised\n",
      "learning aims to solve, such as clustering, anomaly detection, and density\n",
      "estimation.  2. **Key Concepts and Algorithms**:    - Dive into the various\n",
      "algorithms employed in unsupervised learning, including the Hopfield Learning\n",
      "Rule, Boltzmann Learning Rule, Contrastive Divergence, and others like\n",
      "Variational Inference and Gibbs Sampling.    - Study the applications and\n",
      "limitations of each algorithm.  3. **Neural Networks and Probabilistic\n",
      "Methods**:    - Learn about the role of neural networks and probabilistic\n",
      "methods within unsupervised learning.    - Understand the exclusion of\n",
      "backpropagation in unsupervised learning contexts.  4. **Generative Tasks and\n",
      "Discriminative Tasks**:    - Differentiate between generative and discriminative\n",
      "tasks, focusing on their uses in unsupervised and supervised learning\n",
      "respectively.  5. **Advanced Topics**:    - Explore the Engel Classification\n",
      "System and the ILAE Rating Scale within the context of epilepsy, understanding\n",
      "their evaluation by the International League Against Epilepsy.    - Delve into\n",
      "the concept of energy functions in Boltzmann Machines and their analogy to cost\n",
      "functions.  6. **Applications of Unsupervised Learning**:    - Study the\n",
      "practical applications of unsupervised learning in fields like object\n",
      "recognition, image recognition, and automatic target recognition.    -\n",
      "Understand the shift from supervised to unsupervised learning in image\n",
      "recognition and the reasons behind it.  7. **Statistical Methods**:    - Learn\n",
      "about latent variable models and their estimation through the Method of Moments.\n",
      "- Understand the significance of moments, mean vectors, and covariance matrices\n",
      "in unsupervised learning.  8. **Review and Case Studies**:    - Review all the\n",
      "learned concepts through case studies, focusing on real-world applications of\n",
      "unsupervised learning.    - Discuss the impact of unsupervised learning in\n",
      "various domains, including healthcare (e.g., status epilepticus and tonic-clonic\n",
      "seizures) and technology.  9. **Final Assessment**:    - Prepare for the final\n",
      "assessment by revisiting all topics, with special emphasis on understanding the\n",
      "principles behind each unsupervised learning algorithm and its application.\n",
      "This study plan aims to provide a structured approach to mastering the course\n",
      "content, ensuring a deep understanding of both theoretical concepts and\n",
      "practical applications of unsupervised learning.\n"
     ]
    }
   ],
   "source": [
    "print('Structured retrieval model response: \\n')\n",
    "response = structured_chain.invoke({\"question\": demo_question})\n",
    "wrapped_response = textwrap.fill(response, width=80)\n",
    "print(wrapped_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Use natural language and be concise.\n",
    "Answer:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM Generation by running two operations in parallel: retrieve context and passthrough quesiton\n",
    "final_chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": _search_query | retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of the retrieval output fed to the LLM: \n",
      "\n",
      "Search query: Can you help me make a study plan for the course?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured data:\n",
      "Status Epilepticus - RELATED_TO -> Tonic-Clonic Seizures\n",
      "Engel Classification System - EVALUATED_BY -> International League Against Epilepsy\n",
      "Ilae Rating Scale - DEVELOPED_BY -> International League Against Epilepsy\n",
      "Unsupervised Learning - INCLUDES -> Neural Networks\n",
      "Unsupervised Learning - INCLUDES -> Probabilistic Methods\n",
      "Unsupervised Learning - INCLUDES -> Clustering\n",
      "Unsupervised Learning - INCLUDES -> Anomaly Detection\n",
      "Unsupervised Learning - INCLUDES -> Latent Variable Models\n",
      "Unsupervised Learning - EXCLUDES -> Backpropagation\n",
      "Unsupervised Learning - EMPLOYS -> Hopfield Learning Rule\n",
      "Unsupervised Learning - EMPLOYS -> Boltzmann Learning Rule\n",
      "Unsupervised Learning - EMPLOYS -> Contrastive Divergence\n",
      "Unsupervised Learning - EMPLOYS -> Wake Sleep\n",
      "Unsupervised Learning - EMPLOYS -> Variational Inference\n",
      "Unsupervised Learning - EMPLOYS -> Maximum Likelihood\n",
      "Unsupervised Learning - EMPLOYS -> Maximum A Posteriori\n",
      "Unsupervised Learning - EMPLOYS -> Gibbs Sampling\n",
      "Unsupervised Learning - APPLICATION_OF -> Density Estimation\n",
      "Unsupervised Learning - CONTRASTED_WITH -> Supervised Learning\n",
      "Learning - USES -> Mimicry\n",
      "Discriminative Tasks - USES -> Supervised Learning\n",
      "Generative Tasks - USES -> Unsupervised Learning\n",
      "Object Recognition - USES -> Unsupervised Learning\n",
      "Object Recognition - FAVORS -> Supervised Learning\n",
      "Image Recognition - STARTED_WITH -> Supervised Learning\n",
      "Image Recognition - MOVED_TO -> Unsupervised Learning\n",
      "Energy Function - MEASURE_OF -> Network'S Activation State\n",
      "Energy Function - ROLE_IN -> Boltzmann Machines\n",
      "Boltzmann Machines - ANALOGY_WITH -> Cost Function\n",
      "Ludwig Boltzmann - ANALYSIS_OF -> Boltzmann Constant\n",
      "Rbm Network - RELATION_WITH -> Activation Pattern\n",
      "Paul Smolensky - CALLS -> Harmony\n",
      "Hopfield Networks - INSPIRED_BY -> Ferromagnetism\n",
      "Hebbian Learning - RELATED_TO -> Pattern Recognition\n",
      "Hebbian Learning - RELATED_TO -> Experiential Learning\n",
      "Hebbian Learning - PRINCIPLE_OF -> Donald Hebb\n",
      "Hebbian Learning - FUNCTION_OF -> Action Potentials\n",
      "Self-Organizing Map (Som) - PART_OF -> Unsupervised Learning Algorithms\n",
      "Adaptive Resonance Theory (Art) - PART_OF -> Unsupervised Learning Algorithms\n",
      "Adaptive Resonance Theory (Art) - USED_FOR -> Automatic Target Recognition\n",
      "Adaptive Resonance Theory (Art) - USED_FOR -> Seismic Signal Processing\n",
      "Cluster Analysis - PART_OF -> Unsupervised Learning\n",
      "Latent Variable Models - LEARNED_BY -> Method Of Moments\n",
      "Method Of Moments - APPROACH_FOR -> Unsupervised Learning\n",
      "Method Of Moments - ESTIMATES -> Unknown Parameters\n",
      "Unknown Parameters - RELATED_TO -> Moments\n",
      "Moments - OF -> Random Variables\n",
      "Moments - ESTIMATED_FROM -> Samples\n",
      "First Order Moment - IS -> Mean Vector\n",
      "Second Order Moment - IS -> Covariance Matrix\n",
      "Status Epilepticus - RELATED_TO -> Tonic-Clonic Seizures\n",
      "Engel Classification System - EVALUATED_BY -> International League Against Epilepsy\n",
      "Ilae Rating Scale - DEVELOPED_BY -> International League Against Epilepsy\n",
      "Unsupervised Learning - INCLUDES -> Neural Networks\n",
      "Unsupervised Learning - INCLUDES -> Probabilistic Methods\n",
      "Unsupervised Learning - INCLUDES -> Clustering\n",
      "Unsupervised Learning - INCLUDES -> Anomaly Detection\n",
      "Unsupervised Learning - INCLUDES -> Latent Variable Models\n",
      "Unsupervised Learning - EXCLUDES -> Backpropagation\n",
      "Unsupervised Learning - EMPLOYS -> Hopfield Learning Rule\n",
      "Unsupervised Learning - EMPLOYS -> Boltzmann Learning Rule\n",
      "Unsupervised Learning - EMPLOYS -> Contrastive Divergence\n",
      "Unsupervised Learning - EMPLOYS -> Wake Sleep\n",
      "Unsupervised Learning - EMPLOYS -> Variational Inference\n",
      "Unsupervised Learning - EMPLOYS -> Maximum Likelihood\n",
      "Unsupervised Learning - EMPLOYS -> Maximum A Posteriori\n",
      "Unsupervised Learning - EMPLOYS -> Gibbs Sampling\n",
      "Unsupervised Learning - APPLICATION_OF -> Density Estimation\n",
      "Unsupervised Learning - CONTRASTED_WITH -> Supervised Learning\n",
      "Learning - USES -> Mimicry\n",
      "Discriminative Tasks - USES -> Supervised Learning\n",
      "Generative Tasks - USES -> Unsupervised Learning\n",
      "Object Recognition - USES -> Unsupervised Learning\n",
      "Object Recognition - FAVORS -> Supervised Learning\n",
      "Image Recognition - STARTED_WITH -> Supervised Learning\n",
      "Image Recognition - MOVED_TO -> Unsupervised Learning\n",
      "Energy Function - MEASURE_OF -> Network'S Activation State\n",
      "Energy Function - ROLE_IN -> Boltzmann Machines\n",
      "Boltzmann Machines - ANALOGY_WITH -> Cost Function\n",
      "Ludwig Boltzmann - ANALYSIS_OF -> Boltzmann Constant\n",
      "Rbm Network - RELATION_WITH -> Activation Pattern\n",
      "Paul Smolensky - CALLS -> Harmony\n",
      "Hopfield Networks - INSPIRED_BY -> Ferromagnetism\n",
      "Hebbian Learning - RELATED_TO -> Pattern Recognition\n",
      "Hebbian Learning - RELATED_TO -> Experiential Learning\n",
      "Hebbian Learning - PRINCIPLE_OF -> Donald Hebb\n",
      "Hebbian Learning - FUNCTION_OF -> Action Potentials\n",
      "Self-Organizing Map (Som) - PART_OF -> Unsupervised Learning Algorithms\n",
      "Adaptive Resonance Theory (Art) - PART_OF -> Unsupervised Learning Algorithms\n",
      "Adaptive Resonance Theory (Art) - USED_FOR -> Automatic Target Recognition\n",
      "Adaptive Resonance Theory (Art) - USED_FOR -> Seismic Signal Processing\n",
      "Cluster Analysis - PART_OF -> Unsupervised Learning\n",
      "Latent Variable Models - LEARNED_BY -> Method Of Moments\n",
      "Method Of Moments - APPROACH_FOR -> Unsupervised Learning\n",
      "Method Of Moments - ESTIMATES -> Unknown Parameters\n",
      "Unknown Parameters - RELATED_TO -> Moments\n",
      "Moments - OF -> Random Variables\n",
      "Moments - ESTIMATED_FROM -> Samples\n",
      "First Order Moment - IS -> Mean Vector\n",
      "Second Order Moment - IS -> Covariance Matrix\n",
      "Unstructured data:\n",
      "\n",
      "text:  STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.\n",
      "\n",
      "Among neural network models, the self-organizing map (SOM) and adaptive resonance theory (ART) are commonly used in unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing.\n",
      "\n",
      "Probabilistic methods \n",
      "Two of the main methods used in unsupervised learning are principal component and cluster analysis. Cluster analysis is used in unsupervised learning to group, or segment, datasets with shared attributes in order to extrapolate algorithmic relationships. Cluster analysis is a branch of machine learning that groups the data that has not been labelled, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. This approach helps detect anomalous data points that do not fit into either group.\n",
      "\n",
      "A central application of unsupervised learning is in the field of density estimation in statistics, though unsupervised learning encompasses many other domains involving summarizing and explaining data features. It can be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution  conditioned on the label  of input data; unsupervised learning intends to infer an a priori probability distribution .\n",
      "\n",
      "Approaches \n",
      "Some of the most common algorithms used in unsupervised learning include: (1) Clustering, (2) Anomaly detection, (3) Approaches for learning latent variable models. Each approach uses several methods as follows:\n",
      "\n",
      " Clustering methods include: hierarchical clustering, k-means, mixture models, DBSCAN, and OPTICS algorithm\n",
      " Anomaly detection methods include: Local Outlier Factor, and Isolation Forest\n",
      " Approaches for learning latent variable models such as Expectation–maximization algorithm (EM), Method of moments, and Blind signal separation techniques (Principal component analysis, Independent component analysis, Non-negative matrix factorization, Singular value decomposition)\n",
      "\n",
      "Method of moments \n",
      "One of the statistical approaches#Document \n",
      "text: Course Overview:\n",
      "Title: Data-Driven Marketing | Description: In today’s environment, marketing or business analysts require tools and techniques to both quantify the strategic value of marketing initiatives, and to maximize marketing campaign performance. This course aims to teach students concepts, methods and tools to demonstrate the return on investment (ROI) of marketing activities and to leverage on data and marketing analytics to make better and more informed marketing decisions. Course topics covered include marketing performance management, marketing metrics, data management, market response and diffusion models, market and customer segmentation models, analytic marketing and value driven segmentation, digital media marketing analytics, etc. Students will have access to | Subject: Computer Science\n",
      "\n",
      "Related Article 1:\n",
      "Title: Article 1\n",
      "Content: text: User behavior analytics (UBA) is a cybersecurity process about detection of insider threats, targeted attacks, and financial fraud that tracks a system's users. UBA looks at patterns of human behavior, and then analyzes them to detect anomalies that indicate potential threats. Big data platforms like Apache Hadoop are increasing UBA functionality by allowing them to analyze petabytes worth of data to detect insider threats and advanced persistent threats.\n",
      "\n",
      "Purpose \n",
      "UBA's purpose, according to Johna Till Johnson of Nemertes Research, is that \"Security systems provide so much information that it's tough to uncover information that truly indicates a potential for real attack. Analytics tools help make sense of the vast amount of data that SIEM, IDS/IPS, system logs, and other tools gather. UBA tools use a specialized type of security analytics that focuses on the behavior of systems and the people using them. UBA technology first evolved in the field of marketing, to help companies understand and predict consumer-buying patterns. But as it turns out, UBA can be extraordinarily useful in the security context too.\"\n",
      "\n",
      "See also\n",
      " Behavioral analytics\n",
      " Network behavior anomaly detection\n",
      "\n",
      "References\n",
      "\n",
      "External links\n",
      "\n",
      "ABC's Of UBA\n",
      "\n",
      "Software | url: https://en.wikipedia.org/wiki/User%20behavior%20analytics | title: User behavior analytics\n",
      "\n",
      "\n",
      "In addition, the following article provides insights:\n",
      "\n",
      "Related Article 2:\n",
      "Title: Article 2\n",
      "Content: text: Software installed in medical devices is assessed for health and safety issues according to international standards.\n",
      "\n",
      "Safety classes \n",
      "Software classification is based on the potential for hazard(s) that could cause injury to the user or patient.\n",
      "\n",
      "Per [[I#Document \n",
      "text:  article provides insights:\n",
      "\n",
      "Related Article 5:\n",
      "Title: Article 5\n",
      "Content: text: This is a list of mathematics-based methods.\n",
      "\n",
      "Adams' method (differential equations)\n",
      "Akra–Bazzi method (asymptotic analysis)\n",
      "Bisection method (root finding)\n",
      "Brent's method (root finding)\n",
      "Condorcet method (voting systems)\n",
      "Coombs' method (voting systems)\n",
      "Copeland's method (voting systems)\n",
      "Crank–Nicolson method (numerical analysis)\n",
      "D'Hondt method (voting systems)\n",
      "D21 – Janeček method (voting system)\n",
      "Discrete element method (numerical analysis)\n",
      "Domain decomposition method (numerical analysis)\n",
      "Epidemiological methods\n",
      "Euler's forward method\n",
      "Explicit and implicit methods (numerical analysis)\n",
      "Finite difference method (numerical analysis)\n",
      "Finite element method (numerical analysis)\n",
      "Finite volume method (numerical analysis)\n",
      "Highest averages method (voting systems)\n",
      "Method of exhaustion\n",
      "Method of infinite descent (number theory)\n",
      "Information bottleneck method\n",
      "Inverse chain rule method (calculus)\n",
      "Inverse transform sampling method (probability)\n",
      "Iterative method (numerical analysis)\n",
      "Jacobi method (linear algebra)\n",
      "Largest remainder method (voting systems)\n",
      "Level-set method\n",
      "Linear combination of atomic orbitals molecular orbital method (molecular orbitals)\n",
      "Method of characteristics\n",
      "Least squares method (optimization, statistics)\n",
      "Maximum likelihood method (statistics)\n",
      "Method of complements (arithmetic)\n",
      "Method of moving frames (differential geometry)\n",
      "Method of successive substitution (number theory)\n",
      "Monte Carlo method (computational physics, simulation)\n",
      "Newton's method (numerical analysis)\n",
      "Pemdas method (order of operation)\n",
      "Perturbation methods (functional analysis, quantum theory)\n",
      "Probabilistic method (combinatorics)\n",
      "Romberg's method (numerical analysis)\n",
      "Runge–Kutta method (numerical analysis)\n",
      "Sainte-Laguë method (voting systems)\n",
      "Schulze method (voting systems)\n",
      "Sequential Monte Carlo method\n",
      "Simplex method\n",
      "Spectral method (numerical analysis)\n",
      "Variational methods (mathematical analysis, differential#Document \n",
      "text: Spectral method (numerical analysis)\n",
      "Variational methods (mathematical analysis, differential equations)\n",
      "Welch's method\n",
      "\n",
      "See also\n",
      " Automatic basis function construction\n",
      " List of graphical methods\n",
      " Scientific method\n",
      "\n",
      "Methods\n",
      "Scientific method | url: https://en.wikipedia.org/wiki/List%20of%20mathematics-based%20methods | title: List of mathematics-based methods\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print('Example of the retrieval output fed to the LLM: \\n')\n",
    "print(retriever(demo_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid retrieval model response: \n",
      "\n",
      "Search query: Can you help me make a study plan for the course?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! Here's a concise study plan for your Data-Driven Marketing course:\n",
      "1. **Week 1-2: Introduction to Data-Driven Marketing**    - Understand the\n",
      "basics of data-driven marketing.    - Explore marketing performance management\n",
      "and marketing metrics.  2. **Week 3-4: Data Management and Analysis**    - Dive\n",
      "into data management techniques.    - Learn about market response and diffusion\n",
      "models.  3. **Week 5-6: Market and Customer Segmentation**    - Study market and\n",
      "customer segmentation models.    - Practice identifying segments using\n",
      "unsupervised learning methods like clustering.  4. **Week 7-8: Analytic\n",
      "Marketing and Segmentation**    - Explore advanced analytic marketing\n",
      "strategies.    - Understand value-driven segmentation and its application.  5.\n",
      "**Week 9-10: Digital Media Marketing Analytics**    - Learn about digital media\n",
      "marketing analytics.    - Study the role of unsupervised learning in analyzing\n",
      "digital marketing data.  6. **Week 11-12: Leveraging Big Data for Marketing**\n",
      "- Understand the use of big data platforms like Apache Hadoop in marketing\n",
      "analytics.    - Explore user behavior analytics (UBA) for marketing insights.\n",
      "7. **Week 13-14: ROI of Marketing Activities**    - Focus on methods to\n",
      "demonstrate the ROI of marketing activities.    - Study various case studies and\n",
      "apply learned concepts.  8. **Week 15: Final Project**    - Apply all the\n",
      "concepts learned to a real-world marketing problem.    - Present a comprehensive\n",
      "marketing analysis and strategy based on data-driven insights.  Throughout the\n",
      "course: - Engage in practical exercises using datasets to apply concepts. -\n",
      "Participate in discussions and group projects to enhance understanding. -\n",
      "Regularly review and apply statistical methods and unsupervised learning\n",
      "algorithms relevant to marketing analytics.  This plan balances theoretical\n",
      "understanding with practical application, ensuring you gain a comprehensive\n",
      "understanding of data-driven marketing.\n"
     ]
    }
   ],
   "source": [
    "print('Hybrid retrieval model response: \\n')\n",
    "response = final_chain.invoke({\"question\": demo_question})\n",
    "wrapped_response = textwrap.fill(response, width=80)\n",
    "print(wrapped_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: What kind of unsupervised learning is used for data-driven marketing and can you explain more about the connection between data-driven marketing and unsupervised learning?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL () { ... }} {position: line: 1, column: 1, offset: 0} for query: \"CALL { CALL db.index.vector.queryNodes($index, $k, $embedding) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score UNION CALL db.index.fulltext.queryNodes($keyword_index, $query, {limit: $k}) YIELD node, score WITH collect({node:node, score:score}) AS nodes, max(score) AS max UNWIND nodes AS n RETURN n.node AS node, (n.score / max) AS score } WITH node, max(score) AS score ORDER BY score DESC LIMIT $k RETURN reduce(str='', k IN ['text'] | str + '\\\\n' + k + ': ' + coalesce(node[k], '')) AS text, node {.*, `embedding`: Null, id: Null, `text`: Null} AS metadata, score\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In the context of data-driven marketing, unsupervised learning is primarily utilized to uncover hidden patterns and insights from marketing data without pre-existing labels. This approach is crucial for segmenting markets and customers, identifying distinct groups within the data based on shared attributes or behaviors. Techniques such as clustering are commonly employed in this domain to group consumers with similar characteristics or purchasing behaviors, enabling marketers to tailor their strategies more effectively. Additionally, unsupervised learning methods like anomaly detection can help in identifying outliers or unusual patterns in the data, which could signify emerging trends, fraud, or untapped market segments. By leveraging these unsupervised learning techniques, data-driven marketing can achieve more personalized marketing campaigns, efficient targeting, and improved customer understanding, all of which contribute to maximizing the ROI of marketing activities.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_step3b3_2 = \"Can you explain more about the connection between data-driven marketing and unsupervised learning, what kind of unsupervised learning is used for data-driven marketing\"\n",
    "previous_qn = demo_question\n",
    "previous_res = wrapped_response\n",
    "final_chain.invoke(\n",
    "    {\n",
    "        \"question\": question_step3b3_2,\n",
    "        \"chat_history\": [(previous_qn, previous_res)],\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
