{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is to streamline the RAG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3526: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# LangChain's core runnables for orchestrating tasks in workflows\n",
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "# LangChain's core components for building custom prompts, handling messages, and parsing outputs\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Typing Imports\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Integrating LangChain with Neo4j, which can be useful for tasks like combining graph databases and vector stores for advanced AI workflows.\n",
    "# For example:\n",
    "# We can use Neo4jGraph to retrieve structured graph data from Neo4j\n",
    "# We can store and query document embeddings using Neo4jVector\n",
    "# We can leverage LLMGraphTransformer to help the LLM reason about relationships within the graph\n",
    "# We can use remove_lucene_chars to ensure that queries passed into Neo4j are well-formatted and don’t cause issues with search.\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "# Document Loaders and Text Splitters\n",
    "# from langchain.document_loaders import WikipediaLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# LangChain components that interface with OpenAI models\n",
    "# ChatOpenAI handles interactive conversations with a language model\n",
    "# OpenAIEmbeddings transform text into vectors, stores and compares the semantic meaning of user inputs or documents in a vector store like Neo4jVector.\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Neo4j & Graph Visualization\n",
    "# To establish a connection with a Neo4j database and handling the graph database by running Cypher queries, interacting with nodes and relationships\n",
    "from neo4j import GraphDatabase\n",
    "# To visually represent the graph data retrieved from Neo4j\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "\n",
    "# FAISS (Facebook AI Similarity Search) stores text embeddings and then retrieves similar documents based on a query\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Chains for QA by combining a retrieval mechanism (like FAISS) with a language model\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Miscellaneous\n",
    "import os\n",
    "import warnings\n",
    "import textwrap\n",
    "\n",
    "#colab imports if running in Google colab\n",
    "try:\n",
    "  import google.colab\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "except:\n",
    "  pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.nn import CosineSimilarity\n",
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "      <th>Subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Introduction to Business Analytics</td>\n",
       "      <td>This course provides students with an introduc...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business Analytics Immersion Programme</td>\n",
       "      <td>This course aims to equip students with a firs...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Econometrics Modeling for Business Analytics</td>\n",
       "      <td>This course provides the foundations to econom...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Management and Visualisation</td>\n",
       "      <td>This course aims to provide students with prac...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feature Engineering for Machine Learning</td>\n",
       "      <td>This course covers topics that are important f...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>Introduction to Hyperledger Sovereign Identity...</td>\n",
       "      <td>To the surprise of absolutely no one, trust is...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>A System View of Communications: From Signals ...</td>\n",
       "      <td>Have you ever wondered how information is tran...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>Scripting and Programming Foundations</td>\n",
       "      <td>Computer programs are abundant in many people'...</td>\n",
       "      <td>Computer Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>Using GPUs to Scale and Speed-up Deep Learning</td>\n",
       "      <td>Training acomplex deep learning model with a v...</td>\n",
       "      <td>Data Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>The Power of Data</td>\n",
       "      <td>Key course outcomesHigh level overview of the ...</td>\n",
       "      <td>Data Science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1916 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  \\\n",
       "0                    Introduction to Business Analytics   \n",
       "1                Business Analytics Immersion Programme   \n",
       "2          Econometrics Modeling for Business Analytics   \n",
       "3                     Data Management and Visualisation   \n",
       "4              Feature Engineering for Machine Learning   \n",
       "...                                                 ...   \n",
       "1911  Introduction to Hyperledger Sovereign Identity...   \n",
       "1912  A System View of Communications: From Signals ...   \n",
       "1913              Scripting and Programming Foundations   \n",
       "1914     Using GPUs to Scale and Speed-up Deep Learning   \n",
       "1915                                  The Power of Data   \n",
       "\n",
       "                                            Description           Subject  \n",
       "0     This course provides students with an introduc...  Computer Science  \n",
       "1     This course aims to equip students with a firs...  Computer Science  \n",
       "2     This course provides the foundations to econom...  Computer Science  \n",
       "3     This course aims to provide students with prac...  Computer Science  \n",
       "4     This course covers topics that are important f...  Computer Science  \n",
       "...                                                 ...               ...  \n",
       "1911  To the surprise of absolutely no one, trust is...  Computer Science  \n",
       "1912  Have you ever wondered how information is tran...  Computer Science  \n",
       "1913  Computer programs are abundant in many people'...  Computer Science  \n",
       "1914  Training acomplex deep learning model with a v...      Data Science  \n",
       "1915  Key course outcomesHigh level overview of the ...      Data Science  \n",
       "\n",
       "[1916 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'Combined_course_data.csv'\n",
    "course = pd.read_csv(file_path)\n",
    "course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Becurtovirus is a genus of viruses, in the fam...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Becurtovirus</td>\n",
       "      <td>Becurtovirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cyprinivirus is a genus of viruses in the orde...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cyprinivirus</td>\n",
       "      <td>Cyprinivirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Glossinavirus is a genus of viruses, in the fa...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Glossinavirus</td>\n",
       "      <td>Glossinavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ichtadenovirus is a genus of viruses, in the f...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ichtadenovirus</td>\n",
       "      <td>Ichtadenovirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lambdatorquevirus is a genus of viruses, in th...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Lambdatorquevirus</td>\n",
       "      <td>Lambdatorquevirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131044</th>\n",
       "      <td>A non-blanching rash (NBR) is a skin rash that...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Non-blanching%20...</td>\n",
       "      <td>Non-blanching rash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131045</th>\n",
       "      <td>In organic chemistry, the term cyanomethyl (cy...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cyanomethyl</td>\n",
       "      <td>Cyanomethyl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131046</th>\n",
       "      <td>Remaiten is malware which infects Linux on emb...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Remaiten</td>\n",
       "      <td>Remaiten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131047</th>\n",
       "      <td>Gradient-enhanced kriging (GEK) is a surrogate...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Gradient-enhance...</td>\n",
       "      <td>Gradient-enhanced kriging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131048</th>\n",
       "      <td>Cry6Aa (Pesticidal crystal protein Cry6Aa) is ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cry6Aa</td>\n",
       "      <td>Cry6Aa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131049 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0       Becurtovirus is a genus of viruses, in the fam...   \n",
       "1       Cyprinivirus is a genus of viruses in the orde...   \n",
       "2       Glossinavirus is a genus of viruses, in the fa...   \n",
       "3       Ichtadenovirus is a genus of viruses, in the f...   \n",
       "4       Lambdatorquevirus is a genus of viruses, in th...   \n",
       "...                                                   ...   \n",
       "131044  A non-blanching rash (NBR) is a skin rash that...   \n",
       "131045  In organic chemistry, the term cyanomethyl (cy...   \n",
       "131046  Remaiten is malware which infects Linux on emb...   \n",
       "131047  Gradient-enhanced kriging (GEK) is a surrogate...   \n",
       "131048  Cry6Aa (Pesticidal crystal protein Cry6Aa) is ...   \n",
       "\n",
       "                                                      url  \\\n",
       "0              https://en.wikipedia.org/wiki/Becurtovirus   \n",
       "1              https://en.wikipedia.org/wiki/Cyprinivirus   \n",
       "2             https://en.wikipedia.org/wiki/Glossinavirus   \n",
       "3            https://en.wikipedia.org/wiki/Ichtadenovirus   \n",
       "4         https://en.wikipedia.org/wiki/Lambdatorquevirus   \n",
       "...                                                   ...   \n",
       "131044  https://en.wikipedia.org/wiki/Non-blanching%20...   \n",
       "131045          https://en.wikipedia.org/wiki/Cyanomethyl   \n",
       "131046             https://en.wikipedia.org/wiki/Remaiten   \n",
       "131047  https://en.wikipedia.org/wiki/Gradient-enhance...   \n",
       "131048               https://en.wikipedia.org/wiki/Cry6Aa   \n",
       "\n",
       "                            title  \n",
       "0                    Becurtovirus  \n",
       "1                    Cyprinivirus  \n",
       "2                   Glossinavirus  \n",
       "3                  Ichtadenovirus  \n",
       "4               Lambdatorquevirus  \n",
       "...                           ...  \n",
       "131044         Non-blanching rash  \n",
       "131045                Cyanomethyl  \n",
       "131046                   Remaiten  \n",
       "131047  Gradient-enhanced kriging  \n",
       "131048                     Cry6Aa  \n",
       "\n",
       "[131049 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'wikidata.csv'\n",
    "wikidata = pd.read_csv(file_path)\n",
    "wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Course Data:\n",
      "                                             content\n",
      "0  Title: Introduction to Business Analytics | De...\n",
      "1  Title: Business Analytics Immersion Programme ...\n",
      "2  Title: Econometrics Modeling for Business Anal...\n",
      "3  Title: Data Management and Visualisation | Des...\n",
      "4  Title: Feature Engineering for Machine Learnin...\n",
      "\n",
      "Transformed Wikidata:\n",
      "                                             content\n",
      "0  text: Becurtovirus is a genus of viruses, in t...\n",
      "1  text: Cyprinivirus is a genus of viruses in th...\n",
      "2  text: Glossinavirus is a genus of viruses, in ...\n",
      "3  text: Ichtadenovirus is a genus of viruses, in...\n",
      "4  text: Lambdatorquevirus is a genus of viruses,...\n"
     ]
    }
   ],
   "source": [
    "course_transformed = pd.DataFrame({\n",
    "    \"content\": course.apply(lambda row: ' | '.join([f\"{col}: {row[col]}\" for col in course.columns]), axis=1)\n",
    "})\n",
    "\n",
    "# Transform `wikidata` DataFrame to a single-column format\n",
    "wikidata_transformed = pd.DataFrame({\n",
    "    \"content\": wikidata.apply(lambda row: ' | '.join([f\"{col}: {row[col]}\" for col in wikidata.columns]), axis=1)\n",
    "})\n",
    "\n",
    "# Display the transformed tables\n",
    "print(\"Transformed Course Data:\")\n",
    "print(course_transformed.head())\n",
    "\n",
    "print(\"\\nTransformed Wikidata:\")\n",
    "print(wikidata_transformed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01740786,  0.00442912, -0.09215238, ..., -0.02191604,\n",
       "         0.07291625, -0.02235293],\n",
       "       [-0.10091388,  0.0783674 , -0.04533364, ..., -0.1075331 ,\n",
       "         0.04686709,  0.07207245],\n",
       "       [-0.10018466, -0.00640676, -0.0114509 , ..., -0.14957273,\n",
       "         0.06115797,  0.02614287],\n",
       "       ...,\n",
       "       [-0.03868212,  0.05411112,  0.00084907, ...,  0.01953804,\n",
       "        -0.01381   , -0.04266216],\n",
       "       [-0.09186076, -0.1078757 ,  0.04518463, ..., -0.042975  ,\n",
       "        -0.03663828,  0.01403402],\n",
       "       [-0.06280275,  0.0021886 , -0.00058878, ..., -0.0114022 ,\n",
       "        -0.0395432 , -0.0105731 ]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_embeddings_file = 'wiki_title_embeddings.npy'\n",
    "wiki_title_embeddings = np.load(wiki_embeddings_file)\n",
    "wiki_title_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preload your model and wiki_title_embeddings outside the function for efficiency\n",
    "dimension = wiki_title_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(wiki_title_embeddings)\n",
    "\n",
    "def wiki_title_filter_with_course_info(num_candidates, course_info, wikidata_transformed, model = SentenceTransformer('all-MiniLM-L6-v2')):\n",
    "    \"\"\"\n",
    "    Filters the top relevant Wikipedia titles based on the course information.\n",
    "\n",
    "    Parameters:\n",
    "    - num_candidates (int): The number of top candidates to retrieve.\n",
    "    - course_info (str): The course information text.\n",
    "    - wikidata_transformed (DataFrame): The transformed DataFrame containing Wikipedia data.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame containing the top relevant Wikipedia entries.\n",
    "    \"\"\"\n",
    "    # Step 1: Encode the course info to create an embedding\n",
    "    course_embedding = model.encode(course_info)\n",
    "\n",
    "    # Step 2: Search for the most relevant Wikipedia titles using FAISS\n",
    "    _, top_k_indices = faiss_index.search(np.array([course_embedding]), num_candidates)\n",
    "\n",
    "    # Step 3: Filter the top Wikipedia entries\n",
    "    top_wikidata = wikidata_transformed.iloc[top_k_indices[0]].reset_index(drop=True)\n",
    "\n",
    "    return top_wikidata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.nn import CosineSimilarity\n",
    "\n",
    "# Define your utility functions\n",
    "def encode_text(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=64).to(device)\n",
    "    embeddings = model(**inputs).last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "    return embeddings\n",
    "\n",
    "def extract_keywords(content, model):\n",
    "    keywords = model.extract_keywords(content, keyphrase_ngram_range=(3, 3), stop_words='english',\n",
    "                                      use_maxsum=True, nr_candidates=20, top_n=5)\n",
    "    merged_keywords = \" \".join([kw[0] for kw in keywords])\n",
    "    return merged_keywords\n",
    "\n",
    "\n",
    "def refine_user_query_1(query, kw):\n",
    "    return query + \"which has following keywords:\" + kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_candidates(course_info, user_query, top_500_wikidata, tokenizer, query_model, document_model, kw_model, top_n=50):\n",
    "    \"\"\"\n",
    "    Ranks wiki documents based on similarity to the user query and returns the top candidates.\n",
    "\n",
    "    Parameters:\n",
    "    - user_query (str): The user's query.\n",
    "    - top_500_wikidata (DataFrame): DataFrame containing the top 500 filtered wiki data.\n",
    "    - tokenizer (Tokenizer): Tokenizer for encoding text.\n",
    "    - query_model (Model): Model for encoding query text.\n",
    "    - document_model (Model): Model for encoding document text.\n",
    "    - top_n (int): Number of top candidates to return.\n",
    "\n",
    "    Returns:\n",
    "    - List[Dict]: List of dictionaries containing content and similarity score for top candidates.\n",
    "    \"\"\"\n",
    "    # Encode the user query using the query model\n",
    "    merged_keywords = extract_keywords(course_info, kw_model)\n",
    "    user_query = refine_user_query_1(user_query, merged_keywords)\n",
    "    query_embedding = encode_text(user_query, tokenizer, query_model)\n",
    "    \n",
    "    top_candidates = []\n",
    "\n",
    "    # Iterate over each document in the top 500 filtered data\n",
    "    for _, row in top_500_wikidata.iterrows():\n",
    "        # Embed the document content using the document model\n",
    "        doc_embedding = encode_text(row['content'], tokenizer, document_model)\n",
    "        \n",
    "        # Calculate similarity score between query and document embeddings\n",
    "        similarity_score = cosine_sim(query_embedding, doc_embedding).item()\n",
    "        \n",
    "        # Append content and similarity score to the list\n",
    "        top_candidates.append({\n",
    "            \"Content\": row['content'],\n",
    "            \"Similarity Score\": similarity_score\n",
    "        })\n",
    "    \n",
    "    # Sort candidates by similarity score in descending order and select top N\n",
    "    top_candidates = sorted(top_candidates, key=lambda x: x[\"Similarity Score\"], reverse=True)[:top_n]\n",
    "    \n",
    "    return top_candidates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default cross-encoder model for language en\n",
      "Warning: Model type could not be auto-mapped with the defaults list. Defaulting to TransformerRanker.\n",
      "If your model is NOT intended to be ran as a one-label cross-encoder, please reload it and specify the model_type! Otherwise, you may ignore this warning. You may specify `model_type='cross-encoder'` to suppress this warning in the future.\n",
      "Default Model: mixedbread-ai/mxbai-rerank-base-v1\n",
      "Loading TransformerRanker model mixedbread-ai/mxbai-rerank-base-v1 (this message can be suppressed by setting verbose=0)\n",
      "No device set\n",
      "Using device cpu\n",
      "No dtype set\n",
      "Using dtype torch.float32\n",
      "Loaded model mixedbread-ai/mxbai-rerank-base-v1\n",
      "Using device cpu.\n",
      "Using dtype torch.float32.\n"
     ]
    }
   ],
   "source": [
    "from rerankers import Reranker\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the cross-encoder ranker\n",
    "ranker = Reranker('cross-encoder')\n",
    "\n",
    "def rerank_with_cross_encoder(user_query, top_candidates_df, top_n=5):\n",
    "    \"\"\"\n",
    "    Reranks the top articles based on the user query using a cross-encoder model.\n",
    "\n",
    "    Parameters:\n",
    "    - user_query (str): The user's query.\n",
    "    - top_candidates_df (DataFrame): DataFrame containing the initial top candidate articles.\n",
    "    - top_n (int): Number of top articles to return after reranking.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list containing the content of the top re-ranked articles.\n",
    "    \"\"\"\n",
    "    # Prepare the documents and their IDs for the ranking function\n",
    "    docs = top_candidates_df[\"Content\"].tolist()\n",
    "    doc_ids = list(range(len(docs)))\n",
    "\n",
    "    # Use the rank method to get scores and ranks for each document\n",
    "    results = ranker.rank(query=user_query, docs=docs, doc_ids=doc_ids)\n",
    "\n",
    "    # Extract the content of the top N re-ranked articles based on their ranks\n",
    "    top_articles = [result.document.text for result in sorted(results.results, key=lambda x: x.rank)[:top_n]]\n",
    "    \n",
    "    return top_articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def load_text_file(filename):\n",
    "    # Load text data from a .txt file\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from typing import List\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Download NLTK data files (only needs to be done once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize the Sentence Transformer model for semantic similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Use a suitable model\n",
    "\n",
    "def enhanced_split_documents(final_document: str, chunk_size=512, min_chunk_size=256, max_chunk_size=768):\n",
    "    \"\"\"\n",
    "    Splits the final_document into semantically coherent chunks using adaptive chunking.\n",
    "    \n",
    "    Parameters:\n",
    "    - final_document (str): The full text to split.\n",
    "    - chunk_size (int): Target length of each chunk (in words).\n",
    "    - min_chunk_size (int): Minimum length of each chunk to ensure meaningful content.\n",
    "    - max_chunk_size (int): Maximum allowed length of a chunk.\n",
    "    \n",
    "    Returns:\n",
    "    - List[Document]: List of Document objects with split content and metadata.\n",
    "    \"\"\"\n",
    "    # Tokenize the document into sentences\n",
    "    sentences = nltk.sent_tokenize(final_document)\n",
    "\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Count the number of words in the sentence\n",
    "        sentence_length = len(re.findall(r'\\w+', sentence))\n",
    "\n",
    "        # If adding the sentence doesn't exceed the max chunk size, add it\n",
    "        if current_length + sentence_length <= max_chunk_size:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "        else:\n",
    "            # If the current chunk is smaller than min_chunk_size, force add sentences\n",
    "            if current_length < min_chunk_size:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += sentence_length\n",
    "                continue  # Continue adding sentences until we reach min_chunk_size\n",
    "\n",
    "            # Combine sentences into a chunk\n",
    "            chunk_text = ' '.join(current_chunk)\n",
    "            chunk_metadata = {\n",
    "                'start_sentence': i - len(current_chunk),\n",
    "                'end_sentence': i,\n",
    "                'chunk_length': current_length,\n",
    "            }\n",
    "            chunks.append(Document(page_content=chunk_text, metadata=chunk_metadata))\n",
    "\n",
    "            # Start a new chunk with the current sentence\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_length\n",
    "\n",
    "    # Add the last chunk\n",
    "    if current_chunk:\n",
    "        chunk_text = ' '.join(current_chunk)\n",
    "        chunk_metadata = {\n",
    "            'start_sentence': len(sentences) - len(current_chunk),\n",
    "            'end_sentence': len(sentences),\n",
    "            'chunk_length': current_length,\n",
    "        }\n",
    "        chunks.append(Document(page_content=chunk_text, metadata=chunk_metadata))\n",
    "\n",
    "    # Implement semantic overlap\n",
    "    enhanced_chunks = []\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        # Include overlapping sentences based on semantic similarity\n",
    "        if idx > 0:\n",
    "            # Calculate semantic similarity between current chunk and previous chunk\n",
    "            previous_chunk_embedding = model.encode(chunks[idx - 1].page_content)\n",
    "            current_chunk_embedding = model.encode(chunk.page_content)\n",
    "            similarity = cosine_similarity(\n",
    "                [previous_chunk_embedding], [current_chunk_embedding]\n",
    "            )[0][0]\n",
    "\n",
    "            if similarity < 0.7:\n",
    "                # Low similarity, consider adding overlap\n",
    "                overlap_sentences = chunks[idx - 1].page_content.split()[-20:]  # Last 20 words\n",
    "                chunk.page_content = ' '.join(overlap_sentences) + ' ' + chunk.page_content\n",
    "                chunk.metadata['overlap_added'] = True\n",
    "            else:\n",
    "                chunk.metadata['overlap_added'] = False\n",
    "        else:\n",
    "            chunk.metadata['overlap_added'] = False\n",
    "\n",
    "        enhanced_chunks.append(chunk)\n",
    "\n",
    "    return enhanced_chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text_to_file(text, filename=\"final_document.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)\n",
    "    print(f\"Document saved as {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_document(course_info, top_articles):\n",
    "    # Template to create a more readable and coherent document structure\n",
    "    document = f\"Course Overview:\\n{course_info}\\n\\n\"\n",
    "    for i, article in enumerate(top_articles, 1):\n",
    "        document += f\"Related Article {i}:\\n\"\n",
    "        document += f\"Title: Article {i}\\n\"\n",
    "        document += f\"Content: {article}\\n\"\n",
    "        if i < len(top_articles):\n",
    "            document += \"\\n\\nIn addition, the following article provides insights:\\n\\n\"\n",
    "    return document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default cross-encoder model for language en\n",
      "Warning: Model type could not be auto-mapped with the defaults list. Defaulting to TransformerRanker.\n",
      "If your model is NOT intended to be ran as a one-label cross-encoder, please reload it and specify the model_type! Otherwise, you may ignore this warning. You may specify `model_type='cross-encoder'` to suppress this warning in the future.\n",
      "Default Model: mixedbread-ai/mxbai-rerank-base-v1\n",
      "Loading TransformerRanker model mixedbread-ai/mxbai-rerank-base-v1 (this message can be suppressed by setting verbose=0)\n",
      "No device set\n",
      "Using device cpu\n",
      "No dtype set\n",
      "Using dtype torch.float32\n",
      "Loaded model mixedbread-ai/mxbai-rerank-base-v1\n",
      "Using device cpu.\n",
      "Using dtype torch.float32.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "query_model = AutoModel.from_pretrained(\"query_model_scidocs\").to(device)\n",
    "document_model = AutoModel.from_pretrained(\"document_model_scidocs\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_scidocs\")\n",
    "cosine_sim = CosineSimilarity(dim=1)\n",
    "kw_model = KeyBERT()\n",
    "ranker = Reranker('cross-encoder')\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\" # Apply your own key\n",
    "os.environ[\"NEO4J_URI\"] = '' # Apply your own URI\n",
    "os.environ[\"NEO4J_USERNAME\"] = \"neo4j\" # by default or use your own\n",
    "os.environ[\"NEO4J_PASSWORD\"] = '' # Apply your own password\n",
    "\n",
    "graph = Neo4jGraph(url=os.environ[\"NEO4J_URI\"], username=os.environ[\"NEO4J_USERNAME\"], password=os.environ[\"NEO4J_PASSWORD\"]) \n",
    "\n",
    "llm=ChatOpenAI(temperature=0, model_name=\"gpt-4-0125-preview\") # gpt-4-0125-preview occasionally has issues but in theory you would want to use the most capable model to construct the graph\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "\n",
    "    # This line structures the output of the LLM to give a List of names.\n",
    "    names: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"All the course knowledge, teaching material, deliverable, expectation, level and assessment entities \"\n",
    "        \"appear in the text\",\n",
    "    )\n",
    "\n",
    "# Each tuple represents a message with a specific role and content that helps define how different messages should be strucutured\n",
    "# and formatted when interacting with the llm.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are tasked with extracting specific entities from the text. Focus on course knowledge, teaching material, deliverable, expectation, level and assessment entities\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Use the given format to extract information from the following \"\n",
    "            \"input: {question}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combine the prompt template (prompt) with the language model that specifies that the output should be structured in a particular way, specifically to extract entitites.\n",
    "entity_chain = prompt | llm.with_structured_output(Entities)\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_lucene_chars(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove special characters that are not allowed in Lucene queries.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', input)\n",
    "\n",
    "def generate_full_text_query(input: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a full-text search query for a given input string.\n",
    "\n",
    "    This function constructs a query string suitable for a full-text search.\n",
    "    It processes the input string by splitting it into words and appending a\n",
    "    similarity threshold (~2 changed characters) to each word, then combines\n",
    "    them using the AND operator. Useful for mapping entities from user questions\n",
    "    to database values, and allows for some misspellings.\n",
    "    \"\"\"\n",
    "    full_text_query = \"\"\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "\n",
    "    if not words:\n",
    "        return \"\"\n",
    "\n",
    "    for word in words[:-1]:\n",
    "        full_text_query += f\" {word}~2 AND\"\n",
    "    full_text_query += f\" {words[-1]}~2\"\n",
    "\n",
    "    return full_text_query.strip()\n",
    "\n",
    "def structured_retriever(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Collects the neighborhood of entities mentioned\n",
    "    in the question.\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    entities = entity_chain.invoke({\"question\": question})\n",
    "\n",
    "    for entity in entities.names:\n",
    "        # This Neo4j Cypher query performs a full-text search on nodes that have the required label, retrieving the top two matches\n",
    "        # based on the search term provided. After this, the query then looks for relationships that point to or from this entity,\n",
    "        # excluding relationships of type 'MENTIONS'.\n",
    "        response = graph.query(\n",
    "            \"\"\"\n",
    "            CALL db.index.fulltext.queryNodes('entity', $query, {limit: 2})\n",
    "            YIELD node, score\n",
    "            WITH node\n",
    "            MATCH (node)-[r]->(neighbor)\n",
    "            WHERE type(r) <> 'MENTIONS'\n",
    "            RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "            UNION ALL\n",
    "            MATCH (neighbor)-[r]->(node)\n",
    "            WHERE type(r) <> 'MENTIONS'\n",
    "            RETURN neighbor.id + ' - ' + type(r) + ' -> ' + node.id AS output\n",
    "            LIMIT 50\n",
    "            \"\"\",\n",
    "            {\"query\": generate_full_text_query(entity)},\n",
    "        )\n",
    "\n",
    "        # Append results\n",
    "        result += \"\\n\".join([el['output'] for el in response]) + \"\\n\"\n",
    "\n",
    "    return result.strip()\n",
    "\n",
    "# Define a function to combine both structured and unstructred data defined above into a prompt to be fed to the LLM\n",
    "def retriever(question: str):\n",
    "    print(f\"Search query: {question}\")\n",
    "    structured_data = structured_retriever(question)\n",
    "    unstructured_data = [el.page_content for el in vector_index.similarity_search(question)]\n",
    "    final_data = f\"\"\"Structured data:\n",
    "{structured_data}\n",
    "Unstructured data:\n",
    "{\"#Document \". join(unstructured_data)}\n",
    "    \"\"\"\n",
    "    return final_data\n",
    "\n",
    "# Condense a chat history and follow-up question into a standalone question\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question,\n",
    "in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "# Formats chat history to incorporate into a query for the LLM\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),  # Condense follow-up question and chat into a standalone_question\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | ChatOpenAI(temperature=0)\n",
    "        | StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    RunnableLambda(lambda x : x[\"question\"]),\n",
    ")\n",
    "\n",
    "template = \"\"\"You are an AI tutor assisting a student with a study plan for their course.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "First, generate a structured study plan based on the information in the context. Be detailed and clear.\n",
    "\n",
    "Then, provide an explanation about what information in the provided document did you use.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Response:\n",
    "Study Plan:\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt using the revised template\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "\n",
    "# LLM Generation by running two operations in parallel: retrieve context and passthrough quesiton\n",
    "final_chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": _search_query | retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# User query for the study plan\n",
    "user_query = \"Can you help me make a study plan for this course?\"\n",
    "\n",
    "# Initialize a list to store rows of data for the DataFrame\n",
    "results_data = []\n",
    "\n",
    "for i in range(2):\n",
    "    # Randomly select a course document\n",
    "    course_row = course_transformed.sample(n=1).iloc[0]\n",
    "    course_document = course_row['content']\n",
    "    \n",
    "    # Filter the top 500 Wikidata entries related to the course document\n",
    "    top_500_wikidata = wiki_title_filter_with_course_info(500, course_document, wikidata_transformed)\n",
    "    \n",
    "    # Get the top 50 candidates based on similarity scores\n",
    "    top_50_candidates = get_top_candidates(\n",
    "        course_document,\n",
    "        user_query,\n",
    "        top_500_wikidata,\n",
    "        tokenizer,\n",
    "        query_model,\n",
    "        document_model,\n",
    "        kw_model,\n",
    "        50\n",
    "    )\n",
    "    \n",
    "    # Sort and select the top 50 candidates\n",
    "    top_50_candidates_df = pd.DataFrame(top_50_candidates).sort_values(by=\"Similarity Score\", ascending=False).head(50)\n",
    "    \n",
    "    # Rerank the top 50 candidates using the cross-encoder model\n",
    "    top_5_articles = rerank_with_cross_encoder(user_query, top_50_candidates_df, top_n=5)\n",
    "\n",
    "    final_document = construct_document(course_document, top_5_articles)\n",
    "    # Usage example with your existing variables\n",
    "    #documents = enhanced_split_documents(final_document)\n",
    "\n",
    "    raw_documents = [Document(page_content=final_document)]\n",
    "    \n",
    "    # Initialize the TokenTextSplitter\n",
    "    text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=20)\n",
    "\n",
    "\n",
    "\n",
    "    # Split the Document object into smaller chunks\n",
    "    documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
    "\n",
    "    graph_documents = llm_transformer.convert_to_graph_documents(documents)\n",
    "\n",
    "    # To create a new database, you can use Cypher query to delete all nodes and relationships\n",
    "    clear_db_query = \"\"\"\n",
    "    MATCH (n)\n",
    "    DETACH DELETE n\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query to clear the database\n",
    "    graph.query(clear_db_query)\n",
    "\n",
    "    graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    # Ensures that each entity in graph_documents is labeled with its base entity type\n",
    "    baseEntityLabel=True,\n",
    "    # Indicate that the source information (like the original document or context) should be included in the graph nodes or edges.\n",
    "    include_source=True\n",
    "    )\n",
    "\n",
    "    vector_index = Neo4jVector.from_existing_graph(\n",
    "    # Uses a model from OpenAI that converts text into vector embeddings which are used for vector-based search\n",
    "    OpenAIEmbeddings(),\n",
    "    # Search for similar words using a hybrid approach, combining both keyword-based and vector-based searches.\n",
    "    search_type=\"hybrid\",\n",
    "    # Only nodes with the Document label will be indexed\n",
    "    node_label=\"Document\",\n",
    "    # Within the node, we will return the 'text' property\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\"\n",
    "    )\n",
    "\n",
    "    response = final_chain.invoke({\"question\": user_query})\n",
    "    wrapped_response = textwrap.fill(response, width=80)\n",
    "    \n",
    "    print(wrapped_response)\n",
    "\n",
    "    row = {\n",
    "        \"Course Information\": course_document,\n",
    "        \"Response\": wrapped_response\n",
    "    }\n",
    "    \n",
    "    # Append the row data to the results list\n",
    "    results_data.append(row)\n",
    "\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Course Information</th>\n",
       "      <th>Document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Title: Thematic Systems Project I | Descriptio...</td>\n",
       "      <td>### Study Plan for Thematic Systems Project I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title: Implementing Hangman Game Using Basics ...</td>\n",
       "      <td>### Study Plan for Implementing Hangman Game U...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Course Information  \\\n",
       "0  Title: Thematic Systems Project I | Descriptio...   \n",
       "1  Title: Implementing Hangman Game Using Basics ...   \n",
       "\n",
       "                                            Document  \n",
       "0  ### Study Plan for Thematic Systems Project I ...  \n",
       "1  ### Study Plan for Implementing Hangman Game U...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Display the DataFrame to check the results\n",
    "results_df.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
